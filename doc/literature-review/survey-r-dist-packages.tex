\hypertarget{sec:distributedr}{%
    \subsection{DistributedR}\label{sec:distributedr}}

DistributedR offers cluster access for various R data structures,
particularly arrays, and providing S3 methods for a fair range of
standard functions. It has no regular cluster access interface, such as
with \pkg{Hadoop} or MPI, being made largely from scratch.

The package creators have ceased development as of December 2015. The
company, Vertica, has moved on to offering an enterprise database
platform\cite{vertica:_distr}.

\hypertarget{sec:foreach-doc}{%
    \subsection{foreach and doX}\label{sec:foreach-doc}}

\pkg{foreach} offers a high-level looping construct compatible with a variety
of backends\cite{microsoft20}. The backends are provided by other
packages, typically named with some form of ``Do\emph{X}''.
Parallelisation is enabled by some backends, with doParallel allowing
parallel computations\cite{corporation19}, \pkg{doSNOW} enabling cluster
access through the SNOW package\cite{dosnow19}, and doMPI allowing for
direct MPI access\cite{weston17}.

\pkg{foreach} is managed by Revolution Analytics, with many of the Do\emph{X}
corollary packages also being produced by them. Further information of
foreach is given in \cite{weston19:_using}.

I have written more on future in \cref{sec:review-foreach}

\hypertarget{sec:future-furrr}{%
    \subsection{future}\label{sec:future-furrr}}

future captures R expressions for evaluation, allowing them to be passed
on for parallel and ad-hoc cluster evaluation, through the parallel
package\cite{bengtsson20}. Such parallelisation uses the standard MPI or
SOCK protocols.

The author of future is Henrik Bengtsson, Associate Professor at UCSF.
Development on the package remains strong, with Dr.~Bengtsson possessing
a completely full commit calendar and 81,328 contributions on GitHub. I
have written more on future in \cref{sec:review-future}. future has many aspects to it, captured in it's
extensive series of vignettes\cite{bengtsson20:_futur_r}\cite{bengtsson20:_futur_r2}
\cite{bengtsson20:_futur_r3}\cite{bengtsson20:_futur_r4}\cite{bengtsson20:_futur_r5}\cite{bengtsson20:_futur_r6}.

Furrr is a frontend to future, amending the functions from the package
purrr to be compatible with future, thus enabling parallelisation in a
similar form to multicore, though with a tidyverse
style\cite{vaughan18}.

Furrr is developed by Matt Dancho, and Davis Vaughn, an employee at
RStudio.

\hypertarget{sec:parall-snow-mult}{%
    \subsection{Parallel, snow, and multicore}\label{sec:parall-snow-mult}}

Parallel is a package included with R, born from the merge of the
packages \pkg{SNOW} and \pkg{multicore}\cite{core:_packag}. Parallel enables
various means of performing computations in R in parallel, allowing not
only multiple cores in a node, but multiple nodes through \pkg{SNOW}'s
interfaces to MPI and SOCK\cite{tierney18}.

Parallel takes from multicore the ability to perform multicore
processing, with the mcapply function. multicore creates forked R
sessions, which is very resource-efficient, but not supported by
windows.

Support for distributed computing over a simple network of computers is provided by \pkg{SNOW}.
The general architecture of \pkg{SNOW} makes use of a master process that holds the data and launches the cluster, pushing the data to worker processes that operate upon it and return the results to the master.
This is represented in Figure~\cref{fig:snow}

\begin{figure}[H]
\input{img/snow}
\caption{\label{fig:snow}\pkg{SNOW} architecture with four chunks}
\end{figure}

Several different communications mechanisms are made use of by \pkg{SNOW}, including \pkg{PSOCK} and user-created sockets.
Its greatest shortcoming is the lack of persistent data, and the mechanism of distribution employed disallows the usage of very large datasets.

\pkg{multicore} was developed by Simon Urbanek. \pkg{SNOW} was developed by Luke
Tierney, a professor at the University of Iowa, who also originated the
byte-compiler for R

\hypertarget{pbdr}{%
    \subsection{pbdR}\label{pbdr}}

pbdR is a collection of packages allowing for distributed computing with
R\cite{pbdBASEpackage}, with the name being the abbreviation of
Programming with Big Data in R. The packages include high-performance
communication and computation capabilities, including RPC, ZeroMQ, and
MPI interfaces.

The collection is extensive, offering several packages for each of the
main categories of application functionality, communication,
computation, development, I/O, and profiling.

Some selected packages of interest include the following:

\begin{description}

    \item[pbdBASE]
        Includes the base utilities for distributed matrices used in the
        project, including bindings and extensions to
        ScaLAPACK\cite{pbdBASEpackage}.
    \item[\pkg{pbdDMAT}]
        Higher level classes and methods for distributed matrices, including
        manipulation, linear algebra, and statistics routines. Uses the same
        syntax as base R through S4\cite{pbdDMATpackage}.
    \item[pbdMPI]
        Offers a high-level interface to MPI, using the S4 system to program in
        the SPMD style, with no ``master'' nodes\cite{Chen2012pbdMPIpackage}.
    \item[pbdCS]
        A client/server framework for \pkg{pbdR}
        packages\cite{Schmidt2015pbdCSpackage}.
    \item[pbdML]
        Offers machine learning algorithms, consisting at present of only PCA
        and similar linear algebra routines, primarily for demonstration
        purposes\cite{schmidt20}.
    \item[hpcvis]
        Provides profiler visualisations generated by the other profiler
        packages within the collection\cite{hpcvis}.
\end{description}

The project is funded by major government sources and research labs in
the US. In 2016, the project won the Oak Ridge National Laboratory 2016
Significant Event Award; as per \cite{pbdR2012},

\begin{quote}
    OLCF is the Oak Ridge Leadership Computing Facility, which currently
    includes Summit, the most powerful computer system in the world.
\end{quote}

More detail is given in \cite{pbdBASEvignette}.

\hypertarget{sec:rhadoop}{%
    \subsection{RHadoop}\label{sec:rhadoop}}

 \pkg{RHadoop} is a collection of five packages to run \pkg{Hadoop} directly from
R\cite{analytics:_rhadoop_wiki}. The packages are divided by logical
function, including \pkg{rmr2}, which runs MapReduce jobs, and \pkg{rhdfs}, which
can access the HDFS. The packages also include \pkg{plyrmr}, which makes
available plyr-like data manipulation functions, in a similar vein to
{sparklyr}.

It is offered and developed by Revolution Analytics.

\hypertarget{sec:rhipe-deltarho}{%
    \subsection{RHIPE and DeltaRho}\label{sec:rhipe-deltarho}}

RHIPE is a means of ``using \pkg{Hadoop} from R''\cite{deltarho:_rhipe}. The
provided functions primarily attain this through interfacing and
manipulating HDFS, with a function, rhwatch, to submit MapReduce jobs.
The easiest means of setup for it is to use a VM, and for all \pkg{Hadoop}
computation, MapReduce is directly programmed for by the user.

There is currently no support for the most recent version of \pkg{Hadoop}, and
it doesn't appear to be under active open development, with the last
commit being 2015. RHIPE has mostly been subsumed into the backend of
DeltaRho, a simple frontend.

\hypertarget{sparklyr}{%
    \subsection{sparklyr}\label{sparklyr}}

\pkg{sparklyr} is an interface to \pkg{Spark} from within R\cite{luraschi20}. The
user connects to \pkg{Spark} and accumulates instructions for the manipulation
of a \pkg{Spark} DataFrame object using \pkg{dplyr} commands, then executing the
request on the \pkg{Spark} cluster.

Of particular interest is the capacity to execute arbitrary R functions
on the \pkg{Spark} cluster. This can be performed directly, with the
\mintinline{r}{spark_apply()} function, taking a
user-defined function as a formal parameter. It can also be used as part
of a \pkg{dplyr} chain through the \mintinline{r}{mutate()}
function. Extending these, \pkg{Spark}-defined hive functions and windowing
functions are enabled for use in
\mintinline{r}{mutate()} calls. Limitations to
arbitrary code execution include the lack of support for global
references due to the underlying lack in the \pkg{serialize} package.

Some support for graphs and graph manipulation is enabled via usage with
the \pkg{graphframes} package, which follows the Tidyverse pattern of
working solely with dataframes and dataframe derivatives\cite{kuo18}.
This binds to the GraphX component of \pkg{Spark}, enabling manipulation of
graphs in \pkg{Spark} through pre-defined commands.

sparklyr is managed and maintained by RStudio, who also manage the rest
of the Tidyverse (including \pkg{dplyr}).

\hypertarget{sec:sparklyr}{%
    \subsection{SparkR}\label{sec:sparklyr}}

SparkR provides a front-end to \pkg{Spark} from
R\cite{venkataraman20:_spark}. Like \pkg{sparklyr}, it provides the DataFrame
as the primary object of interest. However, there is no support for the
dplyr model of programming, with functions closer resembling base R
being provided by the package instead.

SparkR is maintained directly by Apache \pkg{Spark}, with ongoing regular
maintenance provided. Usage of the package is described in the vignette,
\cite{venktaraman19:_spark_pract_guide}, with implementation
explained in \cite{venkataraman2016sparkr}.

\hypertarget{sec:hmr}{%
    \subsection{hmr}\label{sec:hmr}}

hmr is an interface to MapReduce from R\cite{urbanek20}. It runs super
fast, making use of chunked data. Much of the process is handled by the
package, with automatic R object conversion. hmr integrates with
iotools, of which it is based upon. The author, like that of iotools, is
Simon Urbanek.

\hypertarget{sec:big.data.table}{%
    \subsection{big.data.table}\label{sec:big.data.table}}

\pkg{big.data.table} runs \pkg{data.table} over many nodes in an ad-hoc
cluster\cite{gorecki16}. This allows for big data manipulation using a
data.table interface. The package makes use of Rserve (authored by Simon
Urbanek) to facilitate communication between nodes when running from R.
Alternatively, the nodes can be run as docker services, for fast remote
environment setup, using RSclient for connections. Beyond greater
storage capacity, speed is increased through manipulations on
\pkg{big.data.table}s occurring in parallel. The package is authored by Jan
Gorecki, but hasn't been actively developed since mid-2016.
