\begin{displayquote}
Quod est superius est sicut quod inferius, et quod inferius est sicut quod est superius.

That which is above is like to that which is below, and that which is below is like to that which is above.
\end{displayquote}

In a complex distributed system of coarse or fine grain, I/O by way of inter-node communication leads to major constraints on the individual nodes.
Synchronous blocking I/O is more than just a speed bottleneck; it has the potential to lock and completely prevent essential sections of program execution.
Distributed systems are invariably concurrent at a system-level.
A concurrent system in the macro scale (inter-node) is well supported by concurrency in the micro scale (in-process).

While not utterly essential, concurrency in R would be exceptionally useful in the implementation of the project distributed system.
If lacking, perhaps it can be added.
This chapter follows the investigation into what exists, and what the possibilities are for concurrency in R.
Note that this report follows the definition of concurrency in the strict sense, and is not to be conflated with parallelism, as described by \textcite{pike2012concurrency}.

\subsection{Promises}

Promises are a data structure-based mechanism of handling concurrency\cite{liskov1988promises}.
Promise objects serve to represent operations that may occur at some point in time, with the completion of the operation marking a change in state of the promise object from "unresolved" to "resolved".
Most importantly, promise objects posses a \code{then()} method, which registers a callback to be run at some point following the resolution of the associated promise, itself returning a promise representing the completion of said callback.
Chains of \code{then()}s are then used to cleanly compose concurrent operations.

R has a \pkg{promises} package, clearly influenced by JavaScript's A+ promises\cite{cheng2021promises}.
If it can do as \textit{promised}, much of the problem of concurrency is solved in R.

An initial test is given in \cref{lst:promise-work}.
Note in this listing that there is a very mild race condition in that the prompt is printed in the final line, with the operation of the \code{then()} immediately following.
So far it looks like it is functioning.

\begin{listing}
\begin{minted}{r}
> library(promises)
> p <- promise(function(resolve, reject) resolve(TRUE))
> print(p)
<Promise [fulfilled: logical]>
> then(p, onFulfilled = function(value) print(value))
> [1] TRUE
\end{minted}
\caption{A fully-functioning promise}
\label{lst:promise-work}
\end{listing}

Attempting a simple equivalent outside of an empty stack leads to problems, as shown in \cref{lst:promise-not-work}.
Here, as long as there are frames on the stack, the \code{then()} callback is never triggered.
This may make sense in the avoidance of re-entrancy problems, though it is hardly very well advertised.

\begin{listing}
\begin{minted}{r}
> {function() {
+     p <- promise(function(resolve, reject) resolve(TRUE))
+     print(p)
+     then(p, onFulfilled = function(value) print(value))
+     repeat {}
+ }}()
<Promise [fulfilled: logical]>

^C
> [1] TRUE
\end{minted}
\caption{A non-functioning promise}
\label{lst:promise-not-work}
\end{listing}

Can this be worked around?
Promise domains appear to be an immediate answer to this, however it remains unsatisfactory, due to the requirement that they are eventually called at the top level; forcing evaluation on a busy stack isn't what they were designed for.
Perhaps this can be solved by investigating the reason for this constraint.
If \code{then()}s could just be evaluated anywhere in the stack, race conditions far more serious than a prompt mis-print are certain to manifest.
If it were possible to explicitly state where evaluation may be safe to take place, perhaps this could be avoided.

Analysis of the *promises* source code reveals a dependency on the \pkg{later} package for asynchronous execution, and this is what has lead to the leaky abstraction \cite{chang2021later}.
The central \code{later()} function offered by the package is equivalent to JavaScript's \code{setTimout()} function, and also only executes when there is no other R code on the execution stack.
Operations may be forced in \pkg{later} with \code{run\_now()}, however, and this may serve as an appropriate mechanism for forcing evaluation.
This is shown in Listing \cref{lst:promise-forced}.

\begin{listing}
\begin{minted}{r}
> {function() {
+     p <- promise(function(resolve, reject) resolve(TRUE))
+     print(p)
+     then(p, onFulfilled = function(value) print(value))
+     repeat {
+     later::run\_now()
+     }
+ }}()
<Promise [fulfilled: logical]>
[1] TRUE
\end{minted}
\caption{A forced promise}
\label{lst:promise-forced}
\end{listing}

Jumping down a layer of abstraction does appear to solve this problem.
However, given that \code{run\_now()} must be manually called, a new problem is added; scheduling of \code{then()}s is now partially in userspace.
At this point it is worth looking at alternatives, because the code is starting to become complex, with manual event-loop cycling guaranteed to lead to bugs.

A layer up is the \pkg{coro} package\cite{henry21coro}.
This makes use of \pkg{promises} as part of an emulation of coroutines.
Analysing the source code reveals the creation of a state machine that attempts to replicate R's evaluator, but with allowances for re-entrancy.
This is an impressive piece of work, but to depend a serious platform on such a complex machine with no guarantees of long term support is to massively constrain the platform to very implementation-dependent details.
At this point, it is worth considering custom solutions.

\subsection{Attempted Solution}

An initial attempt at a solution was to use continuations in R to implement promises, with coroutines as a base in a similar manner to how it may be implemented in scheme.
Unfortunately, continuations in R are downwards-only, and so can't be relied upon for proper coroutine or promise implementation.

I attempted a solution without continuations: replicate promises, but have the \code{then()} run at any point, without needing to force it or be on the top level.
This involved the creation of a package that was mostly written in \proglang{C}, to create Promise types, as given in \cref{lst:promise-struct}, and perform their evaluation.

\begin{listing}
\begin{minted}{c}
typedef struct Promise {
    int fd;
    char state[STATE_SIZE+1];
    SEXP value;

    struct Promise *then[MAX_THENS]; /* private */
    int then_i;		             /* private */

    SEXP onFulfilled; /* used only for then */
    SEXP onRejected;  /* used only for then */
} Promise;
\end{minted}
\caption{Internal structure of promises}
\label{lst:promise-struct}
\end{listing}

The intention was to attach it to R's event loop, using the \code{addInputHandler()} function from \code{R\_ext/eventloop.h}.
This didn't achieve the goal, as R's event loop only scans the input handlers at the top level (or when \code{Sys.sleep()}ing), leaving us back to where we started.

At this point I decided to look at how other languages enabled concurrency, as the options had run dry within R.

\subsection{Other Concurrency Paradigms}

Most other mainstream languages have explicit concurrency as a part of the language.
This isn't necessarily a failing of R.
Much of the uptake of concurrency among languages only began in earnest relatively recently, motivated by web servers and services.
Furthermore, R is still at heart a DSL for statistics - it is not teleologically dependent on concurrency as a feature of the language.

Regardless, concurrency has proven to be extremely useful, and it's value is clear in a distributed system;
it is worth exploring further models in detail.

\subsubsection{Threading}

Threading is a very low-level model of preemptive multitasking, where independent "threads" run within a process and are interrupted and context switched by some scheduler, typically the operating system.
R is strictly single-threaded.
Foreign C functions can be called that themselves involve threading, but all direct interaction with R must remain single-threaded.

pthreads\cite{nichols1996pthreads} is the standard option for threading in C for R.
The library offers aspects that may enable concurrency with R, such as mutexes for critical regions involving R code.

However, it remains very low-level, not lending itself well to the rapid prototyping required for research computing.
Furthermore, the scalability of threads is poor, with each thread taking up nontrivial memory, and facing operating system restrictions on total number.

Threads certainly have their place, but can't be depended upon as the central mechanism for concurrency.

\subsubsection{Async/Await}

Async/await is a recent pattern in many programming languages.
The name stems from the syntactic constructs which between them enable cooperative multitasking.
A function defined with an \code{async} keyword may be \code{await}ed upon in certain contexts.
What takes place is that when a scheduler or event loop encounters an \code{await}, the scheduler may be thought of as ``bookmarking'' the position in program execution, and may jump to any other position that is being \code{await}ed, and so on until one of the functions being \code{await}ed returns, whereupon execution will continue from that point to the next \code{await}.
Key examples, though with very different implementations, include \proglang{Python}'s \pkg{asyncio} and \proglang{JavaScript}'s \pkg{async/await}.
The async/await paradigm has had massive uptake, with nearly all major programming languages supporting it.

An example of how async/await may appear within R in the context of the node in the distributed system, is given as \cref{lst:aior}

\begin{listing}
\begin{minted}{r}
# N.B. await sleep immediately prior to non-blocking io is roughly equivalent to awaits on async io

main <- async(function() repeat { # this spins; see main2 for a more efficient implementation
    await(async_sleep(0))
    request <- nonblocking_read()
    if (anything_read(request))
        create_task(process_request(request))
})

process_request <- async(function(request) {
    args <- await_lapply(prereqs(request), emerge)
    result <- tryCatch(do.call(computation(request), args), error=identity) # todo: parallel optimisation
    if (response_needed(request))
        send(address(request),result)
})

emerge <- async(function(id) { # Also spinning
    val <- NULL
    while (is.null(val)) {
        await(async_sleep(0))
        val <- get_from_local_cache()
        if (is.null(val))
            val <- non_blocking_get_from_other_host() # a la aiohttp
    }
    store(id, val)
    val
})

async_run(main())

# Non-spinning; relying on some imaginary external async io functions

main2 <- async(function() repeat {
    request <- await(async_read(in_sock)) # c.f. aiohttp
    if (anything_read(request))
        create_task(process_request(request))
})

emerge2 <- async(function(id) {
    val <- async_gather(async_get_from_local_cache(id),
                        async_get_from_other_host(id))
    store(id, val)
    val
})
\end{minted}
\caption{Imaginary async/await in R}
\label{lst:aior}
\end{listing}


This depends on the ability to re-enter a context in R.
Before exploring the potential for this, there is one other model to be investigated.

\subsubsection{Communicating Sequential Processes}

Communicating Sequential Processes as a model for concurrency stems from Tony Hoare's formal language for describing interaction in a concurrent system\cite{hoare1978communicating}.
In this model, "processes" may be spawned, and can communicate between each other over channels.
Upon sending a message, a process must wait for the receiver to read the message - channels are unbuffered, and the point of execution at a message being received from a sender is known as a "rendezvous" between sender and receiver.
Like async/await, the rendezvous point is a signal for the scheduler or event loop to transfer control to any other point of rendezvous, thereby enabling asynchrony.
The rendezvous is therefore a point of synchronisation, a powerful feature that implicitly sweeps away whole categories of race conditions.
Go has made famous it's goroutines as taking inspiration from CSP\cite{gomem2014}.
More recently, Java is experimentally overhauling it's threading model to use similar concepts in its \pkg{Project Loom}, and GNU Guile has recently included \pkg{fibers} as a variation on CSP as inspired by Concurrent ML.

An example of how a CSP-based concurrency may appear in a node of the distributed system is given in \cref{lst:csp}.
Notably, it is the cleanest and clearest of all the concurrency models assessed thus far.

\begin{listing}
\begin{minted}{r}
main <- function() {

    server <- channel()
    storage <- channel()
    waiting_worker <- channel()
    complete_worker <- channel()
    spawn(routine=serve, channel=server)
    spawn(routine=store, channel=storage)
    repeat {
        select(
            server = function(request) {
                spawn(routine=work, channel=waiting_worker)
                send(channel=waiting_worker, value=c(request, complete_worker))
            },
            complete_worker = function(resolution) {
                send(channel=storage, value=value(resolution))
                if (response_needed(resolution))
                    send(channel=return_address(resolution),
                         value=value(resolution))
            }
        )
    }
}

serve <- function(channel) {
    outside_world <- socket_init()
    repeat send(channel, receive(outside_world))
}

work <- function(channel) {
    send_to_emerger <- function(x) {
        emerger <- channel()
        spawn(routine=emerge, channel=emerger)
        send(channel=emerger, value=x)
        emerger
    }
    request_and_main <- receive(channel)
    request <- request_and_main[[1]]
    main <- request_and_main[[2]]
    prereq_count <- length(prerequisites(request))
    emergers <- lapply(prerequisites(request), send_to_emerger)
    prereqs <- select(list=emergers)
    result <- do.call(computation(request), prereqs)
    send(main, result)
}

store <- function(channel) {
    storage <- new.env()
    repeat {
        item <- receive(channel)
        assign(id(item), value(item), storage)
    }
}

emerge <- function(channel) {
    item <- receive(channel)
    send(channel, GET(id(item), address(item)))
}
\end{minted}
\caption{CSP as imagined in R}
\label{lst:csp}
\end{listing}

\subsubsection{Coroutines}

This all lead to some clarity about cooperative multitasking: Under the hood, it's effectively all coroutines, and these models of asynchrony are largely just variations on access to coroutines.
Jumping in and out of a function at will is exactly what a coroutine offers.
Surprisingly few languages possessed coroutines until recently, in spite of the clear need.
Amusingly, Donald Knuth used a constructed assembly language, MMIX, for his Art of Computer Programming series, with a major contributing factor to this choice being the lack of coroutines in higher-level languages\cite{knuth1}.

\subsection{R will never be concurrent}\label{sec:no}

So, is it fixable?
According to Simon Urbanek of the R core team:

\begin{displayquote}
No.
\end{displayquote}

The implementation of evaluation in R precludes any chance of re-entering contexts, and will remain that way unless there is a near-complete rewrite. Dr. Urbanek elaborates:

\begin{displayquote}
I had a quick look at R and having non-linear contexts is by definition (close to) impossible. The contexts contain many global variables that are stacks themselves, so they can only be unwound linearly down. Hence you cannot have "forks" in contexts due to the stack nature of many things in R (handlers stacks, promise stacks, byte-compiler stacks, ...). To make it even more fun, some are actually local C stack variables, so both R design and C stack is involved here. So allowing this would require some serious re-write of R internals... :/
\end{displayquote}
