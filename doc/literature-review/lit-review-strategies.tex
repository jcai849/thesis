\hypertarget{sec:parallel}{%
	\subsection{Parallelism as a Strategy}\label{sec:parallel}}

The central strategy for manipulating large datasets, from which most
other patterns derive, is parallelisation. To parallelise is to engage
in many computations simultaneously - this typically takes the form of
either task parallelism, wherein tasks are distributed across different
processors; data parallelism, where the same task operates on different
pieces of data across different processors; or some mix of the two.

Parallelisation of a computational process can potentially offer
speedups proportional to the number of processors available to take on
work, and with recent improvements in multiprocessor hardware, the
number of processors available is increasing over time. Most
general-purpose personal computers produced in the past 5 years have
multiple processor cores to enable parallel computation.

Parallelism can afford major speedups, albeit with certain limitations.
Amdahl's law, formulated in 1967, aims to capture the speedup
limitations, with a model derived from the argument given below in
\ref{eq:amdahlsform}\cite{amdahl1967law}\cite{gustafson1988law}:

\begin{equation*}\label{eq:amdahlsform}
	\textrm{Speedup} = \frac{1}{s+\frac{p}{N}}
\end{equation*}

Where,

\begin{description}
	\item[Speedup] total speedup of whole task
	\item[s] time spend by serial processor on inherently serial part of program
	\item[p] time spent by serial processor on parallelisable part of program
	\item[N] number of processors
\end{description}

The implication is that speedup of an entire task when parallelised is
granted only through the portion of the task that is otherwise
constrained by singular system resources, at the proportion of execution
time spent in that task. Thus a measure of skepticism is contained in
Amdahl's argument, with many tasks predicted to show no benefit to
parallelise - and in reality, some likely to slow down with increased
overhead given in parallelisation.

The major response to the skepticism of Amdahl's law is given by
Gustafson's law, generated from timing results in a highly parallelised
system. Gustafson's law presents a scaled speedup as per
\ref{eq:gustafsonsform}

\begin{equation}\label{eq:gustafsonsform}
	\textrm{Scaled speedup} = s' + p'N = N + (1-N)s'
\end{equation}

Where,

\begin{description}
	\item[s'] serial time spent on the parallel system
	\item[p'] parallel time spent on the parallel system
\end{description}

This law implies far higher potential parallel speedup, varying linearly
with the number of processors.

An example of an ideal task for parallelisation is the category of
embarassingly parallel workload. Such a problem is one where the
separation into parallel tasks is trivial, such as performing the same
operation over a dataset independently\cite{foster1995parallel}. Many
problems in statistics fall into this category, such as tabulation,
monte-carlo simulation and many matrix manipulation tasks.

\hypertarget{sec:local}{%
	\subsection{Local Solutions}\label{sec:local}}

While not specifically engaging with larger-than-memory data, a number
of packages take advantage of various parallel strategies in order to
process large datasets efficiently. \textbf{multicore} is one such
package, now subsumed into the \textbf{parallel} package, that grants
functions that can make direct use of multiprocessor systems, thereby
reducing the processing time in proportionality to the number of
processors available on the system.

\textbf{data.table} also makes use of multi-processor systems, with many
operations involving threading in order to rapidly perform operations on
its dataframe equivalent, the data.table.

In spite of all of these potential solutions, a major constraint remains
in that only a single machine is used. As long as there is only one
machine available, bottlenecks form and no redundancy protection is
offered in real-time in the event of a crash or power outage.

The first steps typically taken to manage larger-than-memory data is to
shift part of the data into secondary storage, which generally possesses
significantly more space than main memory.

This is the approach taken by the \textbf{disk.frame} package, developed
by Dai ZJ. \textbf{disk.frame} provides an eponymously named dataframe
replacement class, which is able to represent a dataset far larger than
RAM, constrained now only by disk size\cite{zj20}.

The mechanism of disk.frame is introduced on its homepage with the
following explanation:

\begin{displayquote}
	\{disk.frame\} works by breaking large datasets into smaller individual
	chunks and storing the chunks in fst files inside a folder. Each chunk
	is a fst file containing a data.frame/data.table. One can construct the
	original large dataset by loading all the chunks into RAM and row-bind
	all the chunks into one large data.frame. Of course, in practice this
	isn't always possible; hence why we store them as smaller individual
	chunks. \{disk.frame\} makes it easy to manipulate the underlying chunks
	by implementing dplyr functions/verbs and other convenient functions
	(e.g.~the \mintinline{r}{cmap(a.disk.frame, fn, lazy = F)}
	function which applies the function fn to each chunk of a.disk.frame in
	parallel). So that \{disk.frame\} can be manipulated in a similar
	fashion to in-memory data.frames.
\end{displayquote}

It works through two main principles: chunking, and an array of methods
taking advantage of data.frame generics, including \textbf{dplyr} and
\textbf{data.table} functions.

Another component that isn't mentioned in the explanation, but is
crucial to performance, is the parallelisation offered transparently by
the package.

disk.frames are actually references to numbered \mintinline{r}{fst} files in a
folder, with each file serving as a chunk.
This is made use of through manipulation of each chunk separately,
sparing RAM from dealing with a single monolithic
file\cite{zj19:_inges_data}.
Fst is a means of serialising dataframes, as an alternative to RDS
files\cite{klik19}.
It makes use of an extremely fast compression algorithm developed at
facebook.
Functions are usually mapped over chunks using some functional, but more
complex functions such as those implementing a glm require custom
solutions; as an example the direct modelling function of
\mintinline{r}{dfglm()} is implemented to allow
for fitting glms to the data.
From inspection of the source code, the function is a utility wrapper
for streaming disk.frame data by default into bigglm, a biglm
derivative.
For grouped or aggregated functions, there is more complexity involved,
due to the chunked nature of disk.frame.
When functions are applied, they are by default applied to each chunk.
If groups don't correspond injectively to chunks, then the syntactic
chunk-wise summaries and their derivatives may not correspond to the
semantic group-wise summaries expected.
For example, summarising the median is performed by using a
median-of-medians method; finding the overall median of all chunks'
respective medians.

Therefore, computing grouped medians in disk.frame result in estimates
only --- this is also true of other software, such as spark, as noted in
\cite{zj19:_group_by}.
For parallelisation, future is used as the backend package, with most
function mappings on chunks making use of
\mintinline{r}{future::future_lapply()}
to have each chunk mapped with the intended function in parallel.
future is initialised with access to cores through the wrapper function,
\mintinline{r}{setup_disk.frame()}\cite{zj19:_key}.
This sets up the correct number of workers, with the minimum of workers
and chunks being processed in parallel.
An important aspect to parallelisation through future is that, for
purposes of cross-platform compatibility, new R processes are started
for each worker\cite{zj19:_using}.
Each process will possess its own environment, and disk.frame makes use
of future's detection capabilities to capture external variables
referred to in calls, and send them to each worker.
The strategy taken by \textbf{disk.frame} has several inherent
limitations, however. \textbf{disk.frame} allows only embarassingly
parallel operations for custom operations as part of a
split-apply-combine (MapReduce) pattern.
While there may theoretically be future provision for non-embarrassingly
parallel operations, a significant limitation to real-time operation is
the massive slowdown brought by the data movement from disk to RAM and
back.

\hypertarget{sec:dist}{%
	\section{Distributed Computing as a Strategy}\label{sec:dist}}

The specs of a single contemporary commodity computer are higher than
those that were used in the Apollo lunar landing, yet the management of
large datasets still creates major issues, driven by a simple lack of
capacity to hold them in memory. Supercomputers can surmount this by
holding orders of magnitude higher memory, though only a few
organisations or individuals can bear the financial costs of purchasing
and maintaining a supercomputer.

In a similar form, cloud computing is not a universal solution, owing to
expense, security issues, and data transportation problems. Despite
this, systems rivalling supercomputers can be formed through combining
many commodity computers. An amusing illustration of this was given in
2004, when a flash mob connected hundreds of laptops to attempt running
the linpack benchmark, achieving 180 gigaflops in processing
output\cite{perry2004flashcomp}.

The combination of multiple independent computers to form one cohesive
computing system forms part of what is known as distributed computing.
More serious efforts to connect multiple commodity computers into a
larger computational system is now standard, with software such as
Hadoop and Spark being commonplace in large companies for the purpose of
creating distributed systems.

Distributed systems make possible the real-time manipulation of datasets
larger than a single computer's RAM, by splitting up data and holding it
in the RAM of multiple computers. A factor strongly serving in favour of
distributed computing is that commodity hardware exists in large
quantities in most offices, oftentimes completely unused. This means
that many organisations already have the necessary base infrastructure
to create a distributed system, likely only requiring some software and
configuration to set it all up. Beyond the benefit of pre-existing
infrastructure, a major feature commonly offered by distributed systems,
and lacking in high-powered single computer systems, is that of fault
tolerance - when one computer goes down, as does happen, another
computer in the system had redundant copies of much of the information
of the crashed computer, and computation can resume with very little
inconvenience. A single computer, even very high-powered, doesn't
usually offer fault-tolerance to this degree.

All of the packages examined the above \ref{sec:local} have no
immediate capability to create a distributed system, and have all of the
ease-of-use benefits and all of the drawbacks as discussed.

\hypertarget{distributed-large-scale-computing}{%
	\section{Distributed Large-Scale Computing}\label{distributed-large-scale-computing}}

R does have some well-established packages used for distributed
large-scale computing. Of these, the \textbf{parallel} package is
contained in the standard R image, and encapsulates \textbf{SNOW}
(Simple Network Of Workstations), which provides support for distributed
computing over a simple network of compputers. The general architecture
of \textbf{SNOW} makes use of a master process that holds the data and
launches the cluster, pushing the data to worker processes that operate
upon it and return the results to the master. \textbf{SNOW} makes use of
several different communications mechanisms, including sockets or the
greater MPI distributed computing library. Some shortcomings of the
described architecture is the difficulty of persisting data, meaning the
expense of data transportation every time operations are requested by
the master process. In addition, as the data must originate from the
master (barring generated data etc.), the master's memory size serves as
a bottleneck for the whole system.

The \textbf{pbdR} (programming with big data in R) project provides
persistent data, with the \textbf{pbdDMAT} (programming with big data
Distributed MATrices) package offering a user-friendly distributed
matrix class to program with over a distributed system. It is introduced
on its main page with the following description:

\begin{displayquote}
	The ``Programming with Big Data in R'' project (pbdR) is a set of highly
	scalable R packages for distributed computing and profiling in data
	science. Our packages include high performance, high-level interfaces to
	MPI, ZeroMQ, ScaLAPACK, NetCDF4, PAPI, and more. While these libraries
	shine brightest on large distributed systems, they also work rather well
	on small clusters and usually, surprisingly, even on a laptop with only
	two cores. Winner of the Oak Ridge National Laboratory 2016 Significant
	Event Award for ``Harnessing HPC Capability at OLCF with the R Language
	for Deep Data Science.'' OLCF is the Oak Ridge Leadership Computing
	Facility, which currently includes Summit, the most powerful computer
	system in the world.
\end{displayquote}\cite{pbdR2012}

The project seeks especially to serve minimal wrappers around the BLAS
and LAPACK libraries along with their distributed derivatives, with the
intention of introducing as little overhead as possible. Standard R also
uses routines from the library for most matrix operations, but suffers
from numerous inefficiencies relating to the structure of the language;
for example, copies of all objects being manipulated will be typically
be created, often having devastating performance aspects unless specific
functions are used for linear algebra operations, as discussed in
\cite{schmidt2017programming} (e.g.,
\mintinline{r}{crossprod(X)} instead of \mintinline{r}{t(X) %*% X})

Distributed linear algebra operations in pbdR depend further on the
ScaLAPACK library, which can be provided through the pbdSLAP package
\cite{Chen2012pbdSLAPpackage}. The principal interface for direct
distributed computations is the pbdMPI package, which presents a
simplified API to MPI through R \cite{Chen2012pbdMPIpackage}. All major
MPI libraries are supported, but the project tends to make use of
openMPI in explanatory documentation. A very important consideration
that isn't immediately clear is that pbdMPI can only be used in batch
mode through MPI, rather than any interactive option as in Rmpi
\cite{yu02:_rmpi}.

The actual manipulation of distributed matrices is enabled through the
pbdDMAT package, which offers S4 classes encapsulating distributed
matrices \cite{pbdDMATpackage}. These are specialised for dense matrices
through the \mintinline{r}{ddmatrix} class, though the project offers some
support for other matrices. The \mintinline{r}{ddmatrix} class has nearly all
of the standard matrix generics implemented for it, with nearly
identical syntax for all.

The package is geared heavily towards matrix operations in a statistical
programming language, so a test of its capabilities would quite
reasonably involve statistical linear algebra. An example non-trivial
routine is that of generating data, to test randomisation capability,
then fitting a generalised linear model to the data through iteratively
reweighted least squares. In this way, not only are the basic algebraic
qualities considered, but communication over iteration on distributed
objects is tested.

To work comparatively, a simple working local-only version of the
algorithm is produced in listing \ref{lst:local-rwls}.

\begin{listing}
	\begin{minted}{r}
set.seed(1234)
# Generate the data

n <- 1000
B <- matrix(c(1,3))
x0 <- rep(1, n)
x1 <- rnorm(n, 0, 1)
X <- cbind(x0, x1)
p <- 1 / (1 + exp(- X %*% B))
y <- rbinom(n, 1, p)

# Base comparison
#glm(y ~ x1, family = "binomial")

# RWLS as Newton-Raphson for GLM (logistic regression here)

logReg <- function(X, y, maxIter=80, tolerance=0.01){
	pr <- function(X, B){
		1 / (1 + exp(-X  %*% B))
	}
	##
	weights <- function(X, B, y){
		diag(as.vector(pr(X, B)))
	}
	##
	oldB <- matrix(c(Inf,Inf))
	newB <- matrix(c(0, 0))
	nIter <- 0
	while (colSums((newB - oldB)^2) > tolerance &&
	       nIter < maxIter) {
		oldB <- newB
	## N-R as RWLS
		W <- weights(X, oldB, y)
		hessian <- - t(X) %*% W %*% X
		z <- X %*% oldB + solve(W) %*% (y - pr(X, oldB))
		newB <- solve(-hessian) %*% crossprod(X, W %*% z)
	##
		nIter <- nIter + 1
	}
	newB
}

print(logReg(X, y, tolerance=1E-6, maxIter=100))
\end{minted}
	\caption{Local GLM with RWLS}
	\label{lst:local-rwls}
\end{listing}

It outputs a \(\hat{\beta}\) matrix after several seconds of
computation.

Were pbdDMAT matrices to function perfectly transparently as regular
matrices, then all that would be required to convert a local algorithm
to distributed would be to prefix a \mintinline{r}{dd} to every \mintinline{r}{matrix}
call, and bracket the program with a template as per listing
\ref{lst:bracket}.

\begin{listing}
	\begin{minted}{r}
suppressMessages(library(pbdDMAT))
init.grid()

# program code with `dd` prefixed to every `matrix` call

finalize()
\end{minted}
	\caption{Idealised Common Wrap for Local to Distributed Matrices}
	\label{lst:bracket}
\end{listing}

The program halts however, as forms of matrix creation other than
through explicit \mintinline{r}{matrix()} calls
are not necessarily picked up by that process; \mintinline{r}{cbind} requires a
second formation of a \mintinline{r}{ddmatrix}.

The first issue comes when performing conditional evaluation; predicates
involving distributed matrices are themselves distributed matrices, and
can't be mixed in logical evaluation with local predicates.

Turning local predicates to distributed matrices, then converting them
all back to a local matrix for the loop to understand, finally results
in a program run, however the results are still not accurate.

This is due to
\mintinline{r}{diag()<-}
assignment not having been implemented, so several further changes are
necessary, including specifying return type of the diag matrix as a
replacement.

This serves to outline the difficulty of complete distributed
transparency.

The final working code of pbdDMAT GLM through RWLS is given in listing
\ref{lst:dmat}

\begin{listing}
	\begin{minted}{r}

suppressMessages(library(pbdDMAT))
init.grid()

set.seed(1234)
# Generate the data

n <- 1000
B <- ddmatrix(c(1,3))
x0 <- rep(1, n)
x1 <- rnorm(n, 0, 1)
X <- as.ddmatrix(cbind(x0, x1))
p <- 1 / (1 + exp(- X %*% B))
y <- ddmatrix(rbinom(n, 1, as.vector(p)))

# Base comparison
#glm(y ~ x1, family = "binomial")

# RWLS as Newton-Raphson for GLM (logistic regression here)

logReg <- function(X, y, maxIter=80, tolerance=0.01){
	pr <- function(X, B){
		1 / (1 + exp(-X  %*% B))
	}
	##
	weights <- function(X, B, y){
		diag(as.vector(pr(X, B)), type="ddmatrix")
	}
	##
	oldB <- ddmatrix(c(Inf,Inf))
	newB <- ddmatrix(c(0, 0))
	nIter <- ddmatrix(0)
	maxIter <- as.ddmatrix(maxIter)
	while (as.matrix(colSums((newB - oldB)^2) > tolerance &
	       nIter < maxIter)) {
		oldB <- newB
	## N-R as RWLS
		W <- weights(X, oldB, y)
		hessian <- - t(X) %*% W %*% X
		z <- X %*% oldB + solve(W) %*% (y - pr(X, oldB))
		newB <- solve(-hessian) %*% crossprod(X, W %*% z)
	##
		nIter <- nIter + 1
	}
	newB
}

print(logReg(X, y, tolerance=1E-6, maxIter=100))

finalize()
\end{minted}
	\caption{pbdDMAT GLM with RWLS}
	\label{lst:dmat}
\end{listing}

Decidedly more user-friendly is the \textbf{sparklyr} package, which
meshes \textbf{dplyr} syntax with a \textbf{Spark} backend. Simple
analyses are made very simple (assuming a well-configured and already
running \textbf{Spark} instance), but custom iterative models are
extremely difficult to create through the package in spite of
\textbf{Spark's} support for it.

Given that iteration is cited by a principal author of Spark as a
motivating factor in its development when compared to Hadoop, it is
reasonable to consider whether the most popular R interface to Spark,
sparklyr, has support for
iteration\cite{zaharia2010spark}\cite{luraschi20}. One immediate
hesitation to the suitability of sparklyr to iteration is the syntactic
rooting in dplyr; dplyr is a ``Grammar of Data Manipulation'' and part
of the tidyverse, which in turn is an ecosystem of packages with a
shared philosophy\cite{wickham2019welcome}\cite{wickham2016r}. The
promoted paradigm is functional in nature, with iteration using for
loops in R being described as ``not as important'' as in other
languages; map functions from the tidyverse purrr package are instead
promoted as providing greater abstraction and taking much less time to
solve iteration problems. Maps do provide a simple abstraction for
function application over elements in a collection, similar to internal
iterators, however they offer no control over the form of traversal, and
most importantly, lack mutable state between iterations that standard
loops or generators allow\cite{cousineau1998functional}.

A common functional strategy for handling a changing state is to make
use of recursion, with tail-recursive functions specifically referred to
as a form of iteration in \cite{abelson1996structure}. Reliance on
recursion for iteration is naively non-optimal in R however, as it lacks
tail-call elimination and call stack optimisations\cite{rcore2020lang};
at present the elements for efficient, idiomatic functional iteration
are not present in R, given that it is not as functional a language as
the tidyverse philosophy considers it to be, and sparklyr's attachment
to the the ecosystem prevents a cohesive model of iteration until said
elements are in place.

Iteration takes place in Spark through caching results in memory,
allowing faster access speed and decreased data movement than
MapReduce\cite{zaharia2010spark}. sparklyr can use this functionality
through the \mintinline{r}{tbl_cache()} function to
cache Spark dataframes in memory, as well as caching upon import with
\mintinline{r}{memory=TRUE} as a formal parameter to
\mintinline{r}{sdf_copy_to()}. Iteration can also
make use of persisting Spark Dataframes to memory, forcing evaluation
then caching; performed in sparklyr through
\mintinline{r}{sdf_persist()}.

An important aspect of consideration is that sparklyr methods for dplyr
generics execute through a translation of the formal parameters to Spark
SQL. This is particularly relevant in that separate Spark Data Frames
can't be accessed together as in a multivariable function. In addition,
very R-specific functions such as those from the \textbf{stats} and
\textbf{matrix} core libraries are not able to be evaluated, as there is
no Spark SQL cognate for them.

Canned models are the only option for most users, due to
\textbf{sparklyr's} reliance on Spark SQL rather than the Spark core API
made available through the official \textbf{SparkR} interface.

sparklyr is excellent when used for what it is designed for. Iteration,
in the form of an iterated function, does not appear to be part of this
design.

Furthermore, all references to ``iteration'' in the primary sparklyr
literature refer either to the iteration inherent in the inbuilt Spark
ML functions, or the ``wrangle-visualise-model'' process popularised by
Hadley Wickham\cite{luraschi2019mastering}\cite{wickham2016r}. None of such
references connect with iterated functions.

\hypertarget{other-systems}{%
	\subsection{Other Systems}\label{other-systems}}

In the search for a distributed system for statistics, the world outside
of R is not entirely barren. The central issue with non-R distributed
systems is that their focus is very obviously not statistics, and this
shows in the level of support the platforms provide for statistical
purposes.

The classical distributed system for high-performance computing is MPI.
R actually has a high-level interface to MPI through the \textbf{rmpi}
package. This package is excellent, but extremely low-level, offering
little more than wrappers around MPI functions. For the statistician who
just wants to implement a model for a large dataset, such concern with
minutiae is prohibitive.

Hadoop and Spark are two closely related systems which were mentioned
earlier.

Apache Hadoop is a collection of utilities that facilitates cluster
computing.

Jobs can be sent for parallel processing on the cluster directly to the
utilities using .jar files, ``streamed'' using any executable file, or
accessed through language-specific APIs.

The project began in 2006, by Doug Cutting, a Yahoo employee, and Mike
Cafarella.

The inspiration for the project was a paper from Google describing the
Google File System (described in \cite{ghemawat2003google}), which was
followed by another Google paper detailing the MapReduce programming
model, \cite{dean2004mapreduce}.

Hadoop consists of a file-store component, known as Hadoop Distributed
File System (HDFS), and a processing component, known as MapReduce.

In operation, Hadoop splits files into blocks, then distributes them
across nodes in a cluster (HDFS), where they are then processed by the
node in parallel (MapReduce). This creates the advantage of data
locality, wherein data is processed by the node they exist in.

Hadoop has seen extensive industrial use as the premier big data
platform upon its release.

In recent years it has been overshadowed by Spark, due to the greater
speed gains offered by Spark for many problem sets.

Spark was developed with the shortcomings of Hadoop in mind; Much of
its definition is in relation to Hadoop, which it intended to improve
upon in terms of speed and usability for certain
tasks\cite{zaharia2010spark}.

its fundamental operating concept is the Resiliant Distributed Dataset
(RDD), which is immutable, and generated through external data, as well
as actions and transformations on prior RDD's.

The RDD interface is exposed through an API in various languages,
including R, however it appears to be abandoned to some degree, having
removed from the CRAN repository at 2020-07-10 due to failing checks.

Spark requires a distributed storage system, as well as a cluster
manager; both can be provided by Hadoop, among others.

Spark is known for possessing a fairly user-friendly API, intended to
improve upon the MapReduce interface.

Another major selling point for Spark is the libraries available that
have pre-made functions for RDD's, including many iterative algorithms.

The availability of broadcast variables and accumulators allow for
custom iterative programming.

Spark has seen major use since its introduction, with effectively all
major big data companies having some use of Spark.

In the python world, the closest match to a high-level distributed
system that could have statistical application is given by the python
library \textbf{dask}\cite{rocklin2015dask}. \textbf{dask} offers
dynamic task scheduling through a central task graph, as well as a set
of classes that encapsulate standard data manipulation structures such
as NumPy arrays and Pandas dataframes.

The main difference is that the \textbf{dask} classes take advantage of
the task scheduling, including online persistence across multiple nodes.
\textbf{dask} is a large and mature library, catering to many use-cases,
and exists largely in the Pythonic ``Machine Learning'' culture in
comparison to the R ``Statistics'' culture. Accordingly, the focus is
more tuned to the Python software developer putting existing ML models
into a large-scale capacity. Of all the distributed systems assessed so
far, \textbf{dask} comes the closest to what an ideal platform would
look like for a statistician, but it misses out on the statistical
ecosystem of R, provides only a few select classes, and is tied entirely
to the structure of the task graph.
