\subsection{Introduction}\label{subsec:sparklyr-introduction}

Given that iteration is cited by a principal author of \pkg{Spark} as a motivating factor in its development when compared to \pkg{Hadoop}, it is reasonable to consider whether the most popular \R{} interface to \pkg{Spark}, \pkg{sparklyr}, has support for iteration\cites{zaharia2010spark,luraschi20}.
One immediate hesitation to the suitability of \pkg{sparklyr} to iteration is the syntactic rooting in \pkg{dplyr}; \pkg{dplyr} is a ``Grammar of Data Manipulation'' and part of the tidyverse, which in turn is an ecosystem of packages with a shared philosophy\cite{wickham2019welcome}\cite{wickham2016r}.

The promoted paradigm is functional in nature, with iteration using for loops in \R{} being described as ``not as important'' as in other languages; map functions from the tidyverse purrr package are instead promoted as providing greater abstraction and taking much less time to solve iteration problems.
Maps do provide a simple abstraction for function application over elements in a collection, similar to internal iterators, however they offer no control over the form of traversal, and most importantly, lack mutable state between iterations that standard loops or generators allow\cite{cousineau1998functional}.
A common functional strategy for handling a changing state is to make use of recursion, with tail-recursive functions specifically referred to as a form of iteration in \cite{abelson1996structure}.
Reliance on recursion for iteration is naively non-optimal in \R{} however, as it lacks tail-call elimination and call stack optimisations\cite{rcore2020lang}; at present the elements for efficient, idiomatic functional iteration are not present in R, given that it is not as functional a language as the tidyverse philosophy considers it to be, and \pkg{sparklyr}'s attachment to the the ecosystem prevents a cohesive model of iteration until said elements are in place.

\subsection{Iteration}\label{iteration}

Iteration takes place in \pkg{Spark} through caching results in memory, allowing faster access speed and decreased data movement than MapReduce\cite{zaharia2010spark}.
\pkg{sparklyr} can use this functionality
through the \code{tbl-cache} function to
cache \pkg{Spark} dataframes in memory, as well as caching upon import with
\code{memory-true} as a formal parameter to \code{sdf-copy-to}.

Iteration can also make use of persisting \pkg{Spark} Dataframes to memory, forcing evaluation then caching; performed in \pkg{sparklyr} through \code{sdf-persist}.

The Babylonian method for calculating a square root is a simple iterative procedure, used here as an example.
A standard form in \R{} with non-optimised initial value is given in \cref{lst:basicbab}.

\src{basicbab}{Simple Iteration with the Babylonian Method}

This iterative function is trivial, but translation to \pkg{sparklyr} is not entirely so.

The first aspect that must be considered is that \pkg{sparklyr} works on \pkg{Spark} Data Frames; the variables x and S must be copied to \pkg{Spark} with the aforementioned \code{sdf-copy-to} function.

The execution of the function in \pkg{Spark} is the next consideration, and sparklyr provides two means for this to occur; \code{spark-apply} evaluates arbitrary R code over an entire data frame.
The means of operation vary across \pkg{Spark} versions, ranging from launching and running RScripts in \pkg{Spark} 1.5.2, to Apache Arrow conversion in \pkg{Spark} 3.0.0.

The evaluation strategy of 1.5.2 is unsuitable in this instance as it is excessive overhead to launch RScripts every iteration.

The other form of evaluation is through using \pkg{dplyr} generics, which is what will be made use of in this example.

An important aspect of consideration is that \pkg{sparklyr} methods for \pkg{dplyr} generics execute through a translation of the formal parameters to \pkg{Spark} SQL.
This is particularly relevant in that separate \pkg{Spark} Data Frames can't be accessed together as in a multi-variable function.
In addition, very R-specific functions such as those from the \code{stats} and \code{lib-matrix} core libraries are not able to be evaluated, as there is no \proglang{Spark SQL} cognate for them.
The SQL query generated by the methods can be accessed and ``explained'' through \code{show-query} and \code{explain} respectively; When attempting to combine two \pkg{Spark} Data Frames in a single query without joining them, \code{show-query} reveals that the Data Frame that is referenced through the .data variable is translated, but the other Data Frame has its list representation passed through, which \proglang{Spark SQL} doesn't have the capacity to parse; an example is given in \cref{lst:computer-no} (generated through listing \cref{lst:bad}), showing an attempt to create a new column from the difference between two separate Data Frames

\src{bad}{Attempt in \R{} to form new column from the difference between two separate \pkg{Spark} data frames S and x}

\src{computer-no}{\proglang{Spark SQL} query generated from attempt to form the difference from two separate data frames}

Global variables that evaluate to \proglang{SQL}-friendly objects can be passed and are evaluated prior to translation.
An example is given through \cref{lst:global-ok}, generated through \cref{lst:ok-generator}, where the difference between a variable holding a numeric and a \pkg{Spark} Data Frame is translated into the evaluation of the variable, transformed to a float for \proglang{Spark SQL}, and its difference with the \pkg{Spark} Data Frame, referenced directly.

\src{global-ok}{Spark SQL query generated from attempt to form the difference between a data frame and a numeric}

\src{ok-generator}{Capacity in \pkg{sparklyr} to form new column from the difference between a \pkg{Spark} data frame and a numeric}

A reasonable approach to implementing a Babylonian method in \pkg{sparklyr} is then to combine S and x in one dataframe, and iterate within columns.

\src{sparklyr-bab}{Babylonian method implementation using \pkg{sparklyr}}

\subsection{Conclusion}\label{subsec:sparklyr-conclusion}

\pkg{sparklyr} is excellent when used for what it is designed for.
Iteration, in the form of an iterated function, does not appear to be part of this design; this was clear in the abuse required to implement a simple iterated function in the form of the Babylonian Method.
Furthermore, all references to ``iteration'' in the primary \pkg{sparklyr} literature refer either to the iteration inherent in the inbuilt \pkg{Spark} ML functions, or the ``wrangle-visualise-model'' process popularised by Hadley Wickham\cite{luraschi2019mastering}\cite{wickham2016r}.
None of such references connect with iterated functions.

Thus, it is fair to conclude that \pkg{sparklyr} is incapable of sensible iteration of arbitrary \R{} code beyond what maps directly to SQL; even with mutate, it is a very convoluted interface for attempting any iteration more complex than the Babylonian Method.
Implementation of a GLM function with \pkg{sparklyr} iteration was initially planned, but the point was already proven by something far simpler, and the point is one that did not need to be laboured.

Ultimately, \pkg{sparklyr} is excellent at what it does, but convoluted and inefficient when abused, as when attempting to implement iterated functions.
