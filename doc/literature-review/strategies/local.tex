While not specifically engaging with larger-than-memory data, a number of packages take advantage of various parallel strategies in order to process large datasets efficiently.
\pkg{multicore} is one such package, now subsumed into the \pkg{parallel} package, that grants functions that can make direct use of multiprocessor systems, thereby reducing the processing time in proportionality to the number of processors available on the system.
\pkg{multicore} performs this through parallel execution of \R{} code.

\pkg{data.table} also makes use of multi-processor systems, with many operations involving threading in order to rapidly perform operations on its dataframe equivalent, the \pkg{data.table}.
\pkg{data.table} only implicitly parallelises certain operations, but does not enable general parallelisation.

In spite of all of these potential solutions, a major constraint remains in that only a single machine is used.
As long as there is only one machine available, bottlenecks form and no redundancy protection is offered in real-time in the event of a crash or power outage.

The first steps typically taken to manage larger-than-memory data is to shift part of the data into secondary storage, which generally possesses significantly more space than main memory.

This is the approach taken by the \pkg{disk.frame} package, developed by Dai ZJ.
\pkg{disk.frame} provides an eponymously named dataframe
replacement class, which is able to represent a dataset far larger than
RAM, constrained now only by disk size\cite{zj20}.

The mechanism of \pkg{disk.frame} is introduced on its homepage with the following explanation:

\cqu{zj20}

It is described in greater detail in \cref{sec:disk-frame-study}.
