\R{} does have some well-established packages used for distributed large-scale computing.
Of these, the \pkg{parallel} package is contained in the standard \R{} image, and encapsulates \pkg{SNOW} (Simple Network Of Workstations), which provides support for distributed computing over a simple network of computers.
\pkg{SNOW} is described in greater depth along with the rest of the \pkg{parallel} package in \cref{subsec:parall-snow-mult}.

The \pkg{pbdR} (programming with big data in R) project provides persistent data, with the \pkg{pbdDMAT} (programming with big data Distributed MATrices) package offering a user-friendly distributed matrix class to program with over a distributed system.
It is introduced with the following description:

\cqu{pbdR2012}

The project seeks especially to serve minimal wrappers around the \pkg{BLAS} and \pkg{LAPACK} libraries along with their distributed derivatives, with the intention of introducing as little overhead as possible.
Standard \R{} also uses routines from the library for most matrix operations, but suffers from numerous inefficiencies relating to the structure of the language; for example, copies of all objects being manipulated will be typically be created, often having devastating performance aspects unless specific functions are used for linear algebra operations, as discussed in \textcite{schmidt2017programming} (e.g., \code{crossprod} instead of \mintinline{R}{t(X) %*% X}).

Distributed linear algebra operations in \pkg{pbdR} depend further on the \pkg{ScaLAPACK} library, which can be provided through the \pkg{pbdSLAP} package \cite{Chen2012pbdSLAPpackage}.
The principal interface for direct distributed computations is the \pkg{pbdMPI} package, which presents a simplified API to \pkg{MPI} through \R{} \cite{Chen2012pbdMPIpackage}.
All major MPI libraries are supported, but the project tends to make use of openMPI in explanatory documentation.
A very important consideration that isn't immediately clear is that \pkg{pbdMPI} can only be used in batch mode through \pkg{MPI}, rather than any interactive option as in \pkg{Rmpi} \cite{yu02:_rmpi}.

The actual manipulation of distributed matrices is enabled through the \pkg{pbdDMAT} package, which offers S4 classes encapsulating distributed matrices \cite{pbdDMATpackage}.
These are specialised for dense matrices through the \code{ddmatrix} class, though the project offers some support for other matrices.
The \code{ddmatrix} class has nearly all of the standard matrix generics implemented for it, with nearly identical syntax for all.

The package is geared heavily towards matrix operations in a statistical programming language, so a test of its capabilities would quite reasonably involve statistical linear algebra.
An example non-trivial routine is that of generating data, to test randomisation capability, then fitting a generalised linear model to the data through iteratively reweighted least squares.
In this way, not only are the basic algebraic qualities considered, but communication over iteration on distributed objects is tested.

To work comparatively, a simple working local-only version of the algorithm is produced in \cref{lst:local-rwls}.

\src{local-rwls}{Local GLM with RWLS}

It outputs a \mathfrom{beta-hat} matrix after several seconds of computation.

Were \pkg{pbdDMAT} matrices to function perfectly transparently as regular matrices, then all that would be required to convert a local algorithm to distributed would be to prefix a \code{dd} to every \code{matrix} call, and bracket the program with a template as per listing \cref{lst:bracket}.

\src{bracket}{Idealised Common Wrap for Local to Distributed Matrices}

The program halts however, as forms of matrix creation other than through explicit \code{matrix} calls are not necessarily picked up by that process; \code{cbind} requires a second formation of a \code{ddmatrix}.

The first issue comes when performing conditional evaluation; predicates involving distributed matrices are themselves distributed matrices, and can't be mixed in logical evaluation with local predicates.

Turning local predicates to distributed matrices, then converting them all back to a local matrix for the loop to understand, finally results in a program run, however the results are still not accurate.

This is due to \code{diag-assign} assignment not having been implemented, so several further changes are necessary, including specifying return type of the diagonal matrix as a replacement.

This serves to outline the difficulty of complete distributed transparency.

The final working code of \pkg{pbdDMAT} GLM through RWLS is given in listing \cref{lst:dmat}

\src{dmat}{pbdDMAT GLM with RWLS}

Decidedly more user-friendly is the \pkg{sparklyr} package, which meshes \pkg{dplyr} syntax with a \pkg{Spark} backend.
Simple analyses are made very simple (assuming a well-configured and already running \pkg{Spark} instance), but custom iterative models are extremely difficult to create through the package in spite of \pkg{Spark's} support for it.
More discussion on \pkg{sparklyr} is given in \cref{sec:review-iteration-sparklyr}
