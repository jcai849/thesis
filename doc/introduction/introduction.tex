The rate of growth of datasets continues to outpace attempts to engage meaningfully with them, as individual computer memory limits are increasingly exceeded \cite{kleppmann2017dataintensive}.
At the scale of big data, speed also becomes a constraining factor, with concurrency and parallelism being of increasing importance.
The aim of a statistician seeking to gain novel insight from such datasets commonly includes the interactive use of a complex statistical model, often implemented from scratch using \R{}.
No single system satisfactorily provides the capacity to meet this demand.

Those systems that do come close to meeting the demand provide direction regarding how to gain insight from larger-than-memory datasets.
Most importantly, the standard solution for handling big data is to operate over a distributed system~\cite{boja2012distributed}.
Several systems have seen widespread use within the context of data and machine learning pipelines, such as \pkg{Spark}~\cite{zaharia2016apache} and \pkg{Hadoop}~\cite{shvachko2010hadoop}.
For the statistician mostly familiar with \R{}, these systems provide APIs to \R{} where distributed data may be manipulated and pre-made models fitted.
However, these API's are often found lacking when attempted to be used for the creation of complex statistical models that don't come pre-packaged, due to this not being their primary use-case, and \R{} not being their target language.

Within the motivating context provided, the \lsr{} project has sought to provide a full stack for working with larger-than-memory data in \R{}, allowing the developer to manipulate distributed data and create arbitrary complex, iterative models with which to fit to the data, over a self-contained user-specified computing cluster.

\section{Methods}\label{sec:methods}
\input{doc/introduction/methods}
\section{Results}\label{sec:results}
\input{doc/introduction/results}
\section{Discussion}\label{sec:discussion}
\input{doc/introduction/discussion}
