The initial system implemented for the sake of experiment was a straight-path, naive implementation of a statistical distributed system.
The key defining feature of this system in comparison with latter prototypes, was the master-worker structure of inter-node communication.

\subsection{Network Topology}\label{sec:mw-topo}

\hypertarget{motivation}{%
\subsubsection{Motivation}\label{motivation}}

Central to the implementation of the definitive features of a
distributed system are the forms of structuring the network supporting it.
This is due to the fact that the primary constraint on such a system is
the distribution of data, along with the essential consideration of
message timing between nodes. While the algorithms in this particular
statistical system are equally necessary for functionality in the stated
aim of modelling and manipulation of large data, they are completely
dependent on the structure of the data such algorithms operate upon,
hence the primality placed on the system's information structures.

\hypertarget{overview}{%
\subsubsection{Overview}\label{overview}}

The first questions asked of a distributed system's information
structures relate to it's topology and mechanism of message passing.
This particular system answers that necessarily through it's context and
aims; that it is currently intended to exist as a largely transparent
platform within R, which necessitates interactivity and other standard R
development practices. Thus, a centralised structure must be chosen for
the system, with the node associated with the user's R session acting as
the master. This is the form of operation of other distributed packages
currently existing in R, such as \texttt{sparklyr} and \texttt{SNOW}
\cite{luraschi20}\cite{tierney18}. This can be contrasted with a
decentralised system as in \texttt{pbdR} and it's underlying
\texttt{MPI} implementation \cite{pbdR2012}, which requires R programs
to be written in a manner agnostic to the nodes in which they are
executed upon, resulting in the same program being distributed to all
nodes in the system. In this way, the program is not capable of being
run interactively, something undesirable to the goals of our system. The
master will therefore always be local, all other nodes remote.

\hypertarget{local}{%
\subsubsection{Local}\label{local}}

\hypertarget{description-of-current-system}{%
\subsubsection{Description of Current
System}\label{description-of-current-system}}

As it currently stands, the local information structures are entirely
described by S3 classes, the instances of which act as references to the
payload data being held remotely. These classes are composed as
environments, used for their mutable hash table properties, and contain
three elements:

\begin{description}
\tightlist
\item[\texttt{locs}]
a list of \texttt{Rserve} connections through which the remote payload
data resides in discrete chunks
\item[\texttt{name}]
a UUID character scalar, which corresponds to the symbol which the
chunks are assigned to in the remote environment
\item[\texttt{size}]
an integer vector of the same length as \texttt{locs}, describing the
size (as in \texttt{nrow} or \texttt{length}) of the chunk at each
location
\end{description}

This is coupled with a garbage collection system consisting of a hook to
the removal of the reference object through \texttt{reg.finalize}. Upon
triggering the hook, a directive is issued to all chunks in
\texttt{locs} to remove \texttt{name}, thereby closing the loop between
creation and deletion on local and remote nodes.

\hypertarget{motivation-for-current-system}{%
\subsubsection{Motivation for Current
System}\label{motivation-for-current-system}}

The system exists in it's current form primarily through motivations of
simplicity; minimising complexity in the system until further additions
are required. By themselves, \texttt{loc} and \texttt{name} are
sufficient for referencing any distributed chunks. \texttt{size} is
maintained for the regular need to know lengths of objects as part of
many standard operations, thereby reducing the lookup cost by keeping
the static information locally and directly attached to the reference.

The mechanism of garbage collection is likewise borne of simplicity and
necessity; it requires the least possible steps, and without it,
distributed chunks would accumulate remotely with no means of further
access if their reference is removed, essentially forming a high-level
memory leak.

\hypertarget{insufficiency-of-current-structures}{%
\subsubsection{Insufficiency of Current
Structures}\label{insufficiency-of-current-structures}}

In spite of, and indeed because of, the simple information structure of
the local system, there remain aspects of the design that inhibit the
development of important features, many of them essential. In addition,
clarification in system semantics has revealed a need for greater focus
in areas presently under-served by the system.

A major feature lacking in this system is a global awareness of existing
connections, which can be used in preference to creating new connections
upon instantiation of a distributed object. Take for example, the act of
reading in successive distributed \texttt{csv}s into the system. The
first read takes in file location arguments, among others, then creates
new connections, finally returning a reference. The next read performs
exactly the same actions, and so on. This ignores the highly likely
situation where files are situated in the same locations, and
connections at those locations can be reused, thus potentially saving
from the overhead of extraneous connections and unnecessary data
movement of aligning objects with each other.

Another issue is the closing of connections; as it currently stands,
there is no appropriate garbage collection for connections.

The single name for all chunks also cuts out any possibility of having
multiple chunks belong to the same object referenced via a singular
connection, thereby cutting out a potential mechanism for arbitrary
indexing of objects.

\hypertarget{sec:localproposal}{%
\subsubsection{Proposal for New Structures}\label{sec:localproposal}}

Significant enhancements to the system can be attained through
additional structures addressing the present deficits. Principally, the
introduction of a central table of connections will serve as a single
source of truth, avoiding issues of non-knowledge in creation, deletion,
and usage of channels. This would require a change in the structure of
reference objects, and can consist in changing literal \texttt{RSclient}
channel pointers to identifiers to be searched for in the central
location table. In this way it provides a solution in the manner of the
fundamental theorem of software engineering:

\begin{quote}
All problems in computer science can be solved by adding another level
of indirection \cite{oram2007beautiful}.
\end{quote}

The table slots correspoinding to each identifier may also contain
relevant information to the connection such as host name, rack, etc., in
order to optimise data movement, as well as aid in the decision of
whether or not to create new connections for newly read or instantiated
objects.

Additional improvements, though unrelated, include changes to the
reference classes to allow for globally unique names for each chunk,
which will allow the same connection to house multiple chunks of a
cohesive distributed object, thereby enabling arbitrary indexing
operations. With such changes in structure, garbage collection is able
to be enhanced through centralising the objects of garbage collection
within the central table of locations.

One potential algorithm for garbage collection could involve marking
table elements with chunks to be removed, at their associated channel,
as part of a reference garbage collection hook. The marked objects can
then be used as part of a directive for remote removal at the next
convenience. This can be combined with a reference counter of the number
of extant objects at the referent environment of each channel; upon
complete emptying of the environment, signified by a counter of zero,
that channel itself may then be closed and removed.

\hypertarget{sec:localrel}{%
\subsubsection{Relation to Existing Systems}\label{sec:localrel}}

Most other distributed systems in R require manual specification of a
cluster that then operates either in the background or as an object that
must be retained and manipulated. What is described here bears closer
resemblance to a file system than any particular distributed R package,
with particular relation to the \texttt{UNIX} file system
\cite{ritchie1979evolution}\cite{thompson1974unix}. In the \texttt{UNIX}
file system, files contain no additional information beyond what is
written to them by the user or file generation program. Directories are
also files and consist solely of a regular textual mapping from a file
name to it's entry (\emph{inode}) in a central system table
(\emph{ilist}). The inode contains metadata associated with a file such
as access times and permissions, as well as the physcical address of the
file on disk.

To analogise, references in our system are equivalent to directories.
They provide a mapping from connection names (files) to their entries
(inodes) in a central table (ilist). Furthermore, the table entry
contains an \texttt{RSclient} pointer, analogous to a disk address, as
well as metadata. The form of the metadata differs due to separate
priorities; a list of chunk names in the place of permission bits, etc.
In theory this also allows copies of references to behave as hard links,
though this will introduce major issues involving synchronisation, and
is therefore be avoided for now. This form of garbage collection bears
some resemblance to the file system garbage collection as well, in that
inodes count the number of links to them, issuing a removal directive at
zero links, though our system supplements this through collecting
specific names of chunks for second degree removal. In this manner, the
``marking'' via name collection is closer to the method of marked
garbage collection, in conjunction with reference counting
\cite{knuth1}.

\hypertarget{remote}{%
\subsubsection{Remote}\label{remote}}

\hypertarget{description-and-motivation-for-current-system}{%
\subsubsection{Description and Motivation for Current
System}\label{description-and-motivation-for-current-system}}

The remote end of the system is the simplest component of the entire
setup. Currently, each remote R process is hosted through
\texttt{RServe}, and accessed through \texttt{RSclient} on the local
end. The remote R process holds chunks of data in it's global
environment, and performs whichever operations on that data as are
directed to it from the master R session. The data possesses no more
structure than what was already in the chunk following reading,
operation upon, or reception by the node. This has again been due to
reasons of simplicity, as no presuppositions of structure suggested
themselves at the outset.

\hypertarget{insufficiency-of-current-structures-1}{%
\subsubsection{Insufficiency of Current
Structures}\label{insufficiency-of-current-structures-1}}

The system works very well for something general purpose. However, it
ignores much of the structure inherent in common primitive R objects
such as vectors. For example, to numerically index elements of a
distributed vector, an indexing algorithm currently translates the index
numbers into node-specific indices, and forwards those translated
indices on as part of a call to the relevant nodes. This sees issue when
there are disparate elements at a particular node selected between the
elements of other nodes, and the mechanism for numerical translation
breaks down.

\hypertarget{proposal-for-new-structures}{%
\subsubsection{Proposal for New
Structures}\label{proposal-for-new-structures}}

A potential mechanism for improvement is to attach index attributes
corresponding to the overall index to the chunks. In combination with
remotely-run routines, the local session simply needs to send out a
request for particular indices to all of it's connections, and they can
work out themselves which elements, if any, they correspond to,
returning a vector of elements matched, for us in the creation of a new
reference locally.

This would certainly solve the problem, however it may be redundant to
simply allowing local index translation to account for multiple chunks
at a single connection (as described in section
\ref{sec:localproposal}). It certainly uses significantly more
messaging bandwidth, though by distributing processing of index
translation across nodes, it may be faster in practice. In addition, the
additional structure forced on data chunks by attaching indices is
somewhat contrary to the lack thereof in the analogous \texttt{UNIX}
filesystem described in section \ref{sec:localrel}.

\hypertarget{further-research}{%
\subsubsection{Further Research}\label{further-research}}

Further work involves the actual implementation and assessment of the
proposed information structures, as part of a general rewrite.
Additional research may also involve other garbage collection systems,
with especial interest in file systems, such as those of the Inferno and
Plan 9, distributed operating systems borrowing heavily from
\texttt{UNIX} \cite{dorward1997inferno}\cite{pike1995plan}.

\subsection{Definitive Data Structures}\label{sec:mw-ds}

\hypertarget{motivation}{%
\subsubsection{Motivation}\label{motivation}}

To create a minimal implementation of distributed objects in R, with
transparent operations defined, in order to ascertain relevant
associated issues with further work on distributed computations using R.

\hypertarget{method}{%
\subsubsection{Method}\label{method}}

NOTE: R code located in
\href{github.com/jcai89/phd/src/experiment-eager-dist-obj.R}{experiment-eager-dist-obj.R}

Using the nectar cluster, the \texttt{hdp} node was used as a master
with which to control worker nodes \texttt{hadoop1} through to
\texttt{hadoop8}. RServe was used as the means for control and
communication with the workers. S3 classes were defined for
\texttt{cluster}, \texttt{node}, \texttt{distributed.object} and
\texttt{distributed.vector}. Communication functions operate serially,
but were written with future parallelisation and speed in mind.

The \texttt{node} class contains information on connections to the
worker nodes. The \texttt{cluster} class is a collection of
\texttt{node}s. The cluster is set up using \texttt{make\_cluster},
which \texttt{ssh} into the hosts and launches RServe, along with the
relevant libraries and functions. The global environment local to
specific nodes can be checked with the \texttt{peek} method, serving
purely as a sanity check at present. \texttt{send()} is a generic with
methods defined for the \texttt{node} and \texttt{cluster} classes; it
takes objects from the master node and partitions the objects into
equally sized consecutive pieces and distributes them to the hosts
referenced by the \texttt{cluster} or \texttt{node} objects. It can
equally handle objects with smaller \texttt{split} sizes than there are
nodes, dispersing them maximally. \texttt{send()} is used just to get
data to the nodes to bootstrap the system, and wouldn't be used by the
end-user.

\texttt{distributed.object} at present has no methods defined, serving
as a placeholder for an abstract distributed class.
\texttt{distributed.vector} inherits from \texttt{distributed.object},
and serves as a master reference to data that may be spread across
multiple nodes. It contains a list of hostnames, the indices of the
vector residing on each node, and the name of the vector on the nodes,
typically being a UUID generated with the \texttt{distributed.vector}
creation.

\texttt{receive} is the complement to \texttt{send}, giving a
\texttt{distributed.object} as an argument, and receiving the unsplit
referent of the \texttt{distributed.object} as the value. The method
will have additional usage as a remote version, which would enable
point-to-point communication through a node calling \texttt{receive} on
some distributed object, thereby requesting the referent from its
location on all other nodes. Such remote usage is not yet implemented
due to difficulties with point-to-point communication using RServe.
However, such functionality is essential, and is discussed further in
the successive sections.

As a means of testing operations between \texttt{distributed.vector}
objects, S3 \texttt{Ops} methods were defined, using a complex quoting
function in order to call the correct \texttt{.Generic} and reference
the name of the vectors on the worker nodes. They can interact with
non-distributed objects, with the non-distributed objects being coerced
to distribted. To enable interaction between vectors of different
lengths, some means of alignment must be defined, to allow elements at
equivalent positions to be processed at the same node. This is still to
be implemented, with further discussion given in the next section.

No quality-of-life methods such as \texttt{print} were defined, with
error-checking and special case consideration being kept to a minimum,
due to the primarily exploratory nature of the implementation.

\hypertarget{relevant-points-of-interest}{%
\subsubsection{Relevant Points of
Interest}\label{relevant-points-of-interest}}

Already, the experiment has raised several very important considerations
that had not been noted prior.

Memory management was a particular concern; management of reference and
location of distributed objects emulates memory management at a much
lower level, introducing similar issues to those encountered in
systems-level programming.

The initial distribution of objects raises questions of appropriate
algorithms that take load-balancing and other factors into
consideration. One particular example is the question of what to do with
vectors of different length in their distribution across nodes; if split
equally across nodes, it is unlikely that elements at corresponding
posiions between the vectors, and for operations to take place, a
significant amount of data movement (``shuffling'') will have to take
place. Consideration should be given to forms of distribution that
minimise data movement, perhaps through maximisation of correspondence
with existing vectors, while still avoiding misbalancing node memory.

Memory leaks, not much of a problem at the R level with garbage
collection, return to a potential problem with assignment of distributed
objects being fixed to their local R processes. For example, with the
following code consisting of distributed vectors:
\texttt{c\ \textless{}-\ a\ +\ b}, what occurs is that on every node
\texttt{a} and \texttt{b} exist on, they are summed together, with the
result saved as a new variable with a UUID name; a reference to the name
and locations is then stored locally in the variable \texttt{c}. Were
\texttt{a} and \texttt{b} not to be assigned, however, the result would
still be saved on all of the worker nodes, taking up memory, but without
any local handle for it.

This is a memory leak at a high level, and reassignment is even worse;
conceivably, there could be some side effect for the cases of
non-assignment and reassignment, though this would require a level of
reflection whose existence is currently unclear in R.

Dealing with objects of greater complexity such as matrices are certain
to pose problems, and it is unlikely that whatever evolution of this
implementation would perform better than something that has had years
and teams worth of effort poured into it, such as LAPACK or SCaLAPACK.

The need for data movement between nodes as in the case of aligning two
vectors to exist at equivalent positions at equivalent nodes for the
sake of processing, if it is to be done efficiently, requires
point-to-point communication. The alternative is to have each node
channel data through the master and then on to the appropriate node,
which would be a massive waste of resources. This point-to-point
communication is not so easy to perform in reality, as RServe forks a
fresh R session at every new connection, so objects that exist in a
particular node in connection with the master are not able to be
referenced in any other connection.

\hypertarget{next-steps}{%
\subsubsection{Next Steps}\label{next-steps}}

The next steps in this experiment should involve introducing quality of
life aspects to distributed objects such as formal getters and setters,
before it becomes unmanageable. Further methods for
\texttt{distributed.vector} as well as a generalisation to vectors of
different lengths are necessary. The implementation of operations
between vectors of different lengths requires elements of vectors at
equivalent positions to be on the same node for processing; this implies
some kind of \texttt{align} method, which as discussed in the previous
section, would ideally require point-to-point communication, which isn't
so easily permitted through (ab)using RServe. In turn, some custom
solution would likely be required. Upon implementing this, the system
will be highly flexible, with a clean demonstration of this begging for
the right methods defined such that \texttt{summary} and the like work
smoothly. This would lead naturally to the definition of
\texttt{distributed.data.frame} objects and the like. Furthermore, a
means of reading data from distributed storage to their local R
processes would likely yield very worthwhile insights to the process of
creation of a distributed R system. Porting to S4 may be worthwhile, but
it can be performed later. And finally, a closer literature review on
the issues raised and other solutions will prove very valuable.

\subsubsection{more...}

\hypertarget{introduction}{%
\subsubsection{Introduction}\label{introduction}}

This report serves to document the development status and associated
evaluation of the eager distributed object architecture successive to
the previous report, \href{experiment-eager-dist-obj-pre.html}{The
Precursory Report}. The prior report was necessarily largely
speculative, considerate of a number of potential architectural choices,
with relatively equal weighting; now that non-trivial further
development has taken place, more concrete evaluation can be produced,
and alternatives can be compared against what has been explicitly
observed to work and not work. In order to maintain direction in this
subsequent development, a distributed statistical model was developed
concurrently, making use of the distributed object as described in this
report. This is documented in detail in the report,
\href{experiment-dist-decision-tree.html}{Experiment: Distributed
Decision Tree}. In addition, several new and prior unconsidered aspects
have arisen; following a full description of the current system as well
as elaboration on changes since the previous report, an evaluation of
the architecture is given, with suggestions for future research given as
part of the conclusion.

\hypertarget{sec:overview}{%
\subsubsection{Current System Overview}\label{sec:overview}}

Functions are provided to start the instances through hostnames, to
connect to the servers, and to kill the instances. A ``cluster'' object,
akin to the \emph{SNOW} cluster object, serves as a reference with which
to declare distributed objects upon. Distributed objects exist
conceptually as the actual data split across RServe instances, and a
reference containing the minimum of necessary information existing on
the users R session. The users R session thus serves as the master node
in this distributed system. Importantly, the object references are
registered with \texttt{reg.finalize} to run a cleanup on the hosts of
the data when being garbage collected themselves. Distributed objects
are formed from two possible sources; master side, or slave side.
Formation on the master side involves taking an existing R object,
splitting it up according to some function (currently splitting
according to most even element distribution), and sending the splits to
their associated RServe instances, described in a supplied cluster
object formal parameter to the creation function. All of the splits are
assigned a name on the server side, stored in the distributed object
reference. It is an unlikely user-end scenario that the big data is
small enough to fit on the host in order to send it, so reading in data
on the worker end, then feeding information back to produce a reference,
is the other means of producing a distributed object. At present, this
takes the form of a \texttt{read.distributed.csv} call that forwards on
most of the arguments to server-side \texttt{read.csv} functions. The
nodes then measure the number of rows on their chunk, sending back the
information to inform the creation of a reference. Beyond the storage of
data larger than the user-end memory, distributed objects can mimic
standard R objects, with an example being distributed vectors having
\texttt{Ops} defined. Operations between vecto \#forwarded on to the
hosts, along with the names of their associated chunks, for the hosts to
perform at the distributed end.\} The results are assigned to a provided
UUID, and a distributed vector reference is returned on the master
session, pointing to the new chunks that have been created under the ID.
Operations between distributed vectors and non-distributed vectors take
place through distributing the non-distributed vectors, and recursing on
the operation call, along with the original distributed vector and the
newly created distributed vector. An important aspect to operations
between vectors is that the actual processing of operations at
corresponding locations between vectors necessarily requires the
relevant elements to exist on the same RServe instance. This creates a
complication in operations between vectors of different lengths, or
distributed vectors with corresponding elements on different nodes. In
this case the scalar is distributed to all of the hosts of the
distributed vector, with the distributed end running the operation on
the chunk and scalar, providing recycling equivalent to a
non-distributed object due to scalar length being a multiplicative
identity over the length of a vector. Beyond operations between vectors,
some common functions of vector formal parameters have been implemented
for distributed vectors. The nature of the returned object is of great
importance in providing consistency and transparency, along with
reasonable performance; such goals are often in tension against each
other.

Functions returning distributed vectors are arguably the more
transparent among the functions acting on distributed vectors, in the
sense that they return the same degree of distributution as their input,
as regular R functions implicitly do. Such functions include the
\texttt{head} function as well as simple surjective functions.

Data often requires a secondary function acting in a reductive capacity
over combined data resulting from initial node-specific operations. The
resultant algorithms are typically not possible to entirely parallelise,
and the reductive locus in this distributed system is set to be the
user-side master R session. As an example, the \texttt{unique} function
attains the unique elements within an array; it is implemented in
MapReduce form of \texttt{unique} operations within each node, followed
by a \texttt{unique} over their combined output within the master node.
In this way, the amount of data moved is minimised through initial
reduction. As the resulting data exists in a non-distributed form on the
master, it is returned in that form without forcing distribution. It is
a trivial matter to redistribute the data, but this is not presumed on
behalf of the system.

Data Frames are implemented in distributed form in a very similar manner
to vectors, their only difference being a row-wise splitting, compared
to and element-wise splitting. On the nodes, the data frame chunks are
in fact fully functional data frames, along with the full set of
attributes.

Selection and subsetting of distributed data frames and vectors are
limited to subsetting by distributed and non-distributed logical
vectors, and local consecutive numeric vectors. Specification of columns
for data frames can take any standard form.

Of the different forms, subsetting by a distributed logical vector is
the most straightforward; simply directing each node to subset the
node-local object chunk by the node-local logical vector chunk, assuming
alignment of the two. The resulting subset is then measured (if even
existing), with the information sent back to the master as a new
distributed object reference.

Subsetting through a local logical vector requires distributing the
vector in alignment to the object, then carrying out subsetting as
described by a distributed logical vector.

Local consecutive numeric vectors rely on the consecutive nature of
object distribution in their subsetting of objects. The elements of the
vector are compared to the known object indices on each host, then
translated into the equivalent node-specific indices for each element.
To illustrate: a host containing elements \texttt{100} to \texttt{150}
will have the example selection \texttt{100:130} translated to
\texttt{1:30}, as these are the equivalent elements relative to that
specific node.

\hypertarget{description-of-system-changes}{%
\subsubsection{Description of System
Changes}\label{description-of-system-changes}}

In comparison with the previous report, the changes made are fairly
extensive. Beyond the obvious additions alluded to in
{[}@sec:overview{]} that weren't previously present, some more notable
changes are described in this section.

Memory was considered a significant issue in the system at the time of
the previous report. The primary issues have been hugely reduced through
the usage of server-side UUID's for chunk assignment, and finalizer
functions to remove the remote chunks upon master-side garbage
collection. This massively reduces the virtual memory leaks, but some
issues remain, discussed in section {[}@sec:eval{]}

Previous operations on chunks took place sequentially within an
\texttt{lapply}. This has been replaced with a semi-parallel function
where the operations are sent as directives to the nodes without waiting
for the completion of the respective operations. This is then
immediately followed by a makeshift barrier function which effectively
polls for completion of all the operations. Regardless of the form of
these operations, issues still remain, again discussed in section
{[}@sec:eval{]}.

Significant steps towards interaction between objects of different
shapes and locations have taken place, with recycling of length one
vectors in operations and space for a formal alignment method now
implemented.

Perhaps most significant change in terms of user experience, though only
a surface-level layer in implementation, manual communication syntax has
been reduced in favour of new syntax that adheres closer to a
declarative and more R-like paradigm. Concretely, \texttt{send} and
\texttt{receive} have been replaced with \texttt{as.distributed} and the
empty subset function (\texttt{{[}{]}}) respectively. This is matched
with an increasing developmental attitude of user-end simplicity.

\hypertarget{sec:eval}{%
\subsubsection{Architectural Evaluation}\label{sec:eval}}

The experimental system has yielded significant information at this
point such that judgements can be made on successes and failures, as
well as possible solutions.

The central success of this approach is that to a large extent it does
actually work. As a case in point, the Bureau of Transportation's
\emph{flights} {[}@bot2009flights{]} dataset was loaded in its entireity
as a distributed data frame, taking up a combined 17Gb and 119 million
rows. This was able to be manipulated and subset in great speed, along
with operations and reductive functions such as \texttt{table} run upon
it.

In close second to this success is the transparency inherent in the
system. Every single operation exactly mirrors the standard R syntax for
equivalent operations, beyond the cluster setup and act of distribution.

There still remain some important issues to solve before claiming any
major success to such an approach; beyond these, some issues appear
inherent to this approach to a distributed system, and will inform later
experimentation in other forms of distributed systems.

The act of object creation through operations on existing distributed
objects has already been the source of some issue. First, a success in
this regard has been the lack of a need for cluster-specific directives
from the user in the creation, taking instead the information already
encapsulated in the existing objects to form the necessary data to be
used in the generation of new distributed objects. However, the
disconnect between creating an object in the cluster and returning a
handle has already lead to errors; interruption in the space within this
disconnect through errors or even garbage collection can lead to objects
still being created on the worker nodes, but no handle returned. This is
another virtual memory leak, with a possible solution being to
encapsulate existence of the chunks of an object within its reference,
with a \texttt{future}-like \texttt{resolve} attribute indicating this.

Interaction of non-aligned vectors is lacking beyond the few cases
implemented, such as the recycling of length one vectors. This leaves
significant room for improvement, along with many potential means to
enact this improvement. The resolution of this issue would result in two
immediate features: non-consecutive numerical indexing, as well as
smoother operations between two objects existing in different locations
or shapes. A possible solution to this is the encapsulation of
row-number with chunks, and requests for selections or alignments being
sent to all hosts, with only those containing the relevant elements
responding.

The return type of functions on distributed vectors is not very
consistent, as alluded to in {[}@sec:overview{]}. Some operations yield
distributed objects, while others yield local. It is fairly clear from
an implementation perspective which return type is simplest, however
from a user perspective it only appears inconsistent. It is a trivial
matter to distribute the local objects, thereby setting all functions to
return distributed, but it a potentially unnecessary and costly
operation.

A very common operation is to check the type of objects in R, including
related operations such as checking \texttt{is.na} and
\texttt{all.equal}. When interacting with distributed objects, it is not
entirely clear how this should be performed. For example, the class of
every distributed vector is \texttt{distributed.vector}. This is the
case regardless of the class of the underlying chunks that make it up.
Even if the chunks are all logical, a call to \texttt{is.logical} will
return false on the distributed vector, as it should. However, this
leads to breaking transparency, as an equivalent non-distributed vector
will yield true. A possible solution could be to have finer distributed
vector classes that couple with their underlying types, and inherit from
a parent \texttt{distributed.vector} class.

The nature of the connections to RServe instances have proven
exceedingly efficient to work with, and have performed extremely well
despite not being used as designed. The problem of inter-node
communication was already covered in the initial report. In addition to
this, some further issues have appeared, including the difficulty of
debugging errors occurring at the server end, as well as occasional and
somewhat unpredictable connections dropping \footnote{It appears somehow
  related to the occurance of thrashing on the server side}. Likely, a
more mature distributed object system will require the creation of
something like an app manager at some point, replacing the current
RServe form of connection. In the mean time, an abstraction layer for
communication will ease any potential transition to such a communication
system.

In a similar vein to the connection issues, part of the problem is that
there always will be uncontrollable connection issues in some form, and
most distributed systems take this into account, offering varying
degrees of resilience to hardware and connection failure. It would be
worth considering how this can be integrated into this experiment, or
whether it is of a scale to be an experiment of its own; regardless, it
is possibly time to start taking note of this.

Finally, speed is something that certainly has plenty of room for
improvement. Premature optimisation was very consciously avoided, being
the root of all evil and all. In spite of this, many operations, such as
selection and subsetting, take place extremely fast due to the design of
the system. Some operations are significantly slower, such as
\texttt{table}, to the point of slowing down the distributed decision
tree to unusability for big data. With profiling and optimisation this
can very likely be significantly improved.

\hypertarget{conclusion}{%
\subsubsection{Conclusion}\label{conclusion}}

The experiment has yielded very functional distributed vector and data
frame objects, and raised a high volume of information relating to
optimal construction, interaction, and architecture of such a system.
Key successes as well as failings have been enumerated, mostly paired
with potential solutions. The volume of issues and solutions created
since the initial report demonstrates the success of the experiment thus
far in exploring the nature of this specific eager distributed object
approach to large-scale statistical modelling using R.