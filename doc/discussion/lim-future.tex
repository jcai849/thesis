The simplicity and flexibility of the framework may very well be \lsr{}'s greatest strengths however, in overcoming limitations relative to an imaginary improved version of itself.
The foremost limitation is the lack of robustness when faced with machine or network failure.
This must be addressed through some form of self-healing data.
What future work might practically enable this?
One possible solution is described in \cref{sec:self-heal}.

\subsection{Self Healing Data}\label{sec:self-heal}

As described in \cref{sec:dag}, a lineage for each chunk is stored alongside it.
What this may imply for checkpointing is explored in this section.

In order to back up data, which may be later restored in the face of machine failure, checkpoints of individual chunks may be taken\cite{elnozahy2002recovery}.

For all practical purposes, not every single chunk can be checkpointed.
The amount of time spent writing to disk, or replicating across machines, is significant and will slow the system.
As such, the non-trivial decision of which chunks to checkpoint, and how to to restore from sparse checkpoints, serves as the basis of this discussion.

A variety of mechanisms may be used to trigger the designation of a node as a checkpoint, as well as methods of checkpointing.
The methods of checkpointing include redundancy along the cluster, or dumping the data structure to disk; dumping is favoured in this discussion due to the easier reasoning, but either could be considered or even combined without loss in generality\cite{walters2009replication}.
Triggers for checkpointing may be classified into whether they occur at the creation of the node, or during its lifetime.

Given that each node retains a reference to its direct prerequisites, information on the dependencies is easily accessed, and may be propagated along the graph as each new node is added.
This fact can be taken advantage of in order to implement creation-based checkpointing, in aid of fault-tolerance.

I suggest a time-to-recover checkpointing scheme, which attempts to checkpoint based on reaching a certain limit for how long it would take for the system to recover in the face of a worst-case fault, such as a total power outage.
This is performed through recording how long it takes to independently generate each chunk, and recording that information in the node associated with the chunk.
Each chunk also takes the maximum generation time from each of its direct prerequisites and adds that to its own time, in order to create a cumulative generation time, in a similar fashion to a Merkle tree or blockchain where properties of nodes accumulate along references, though without the cryptographic properties\cites{merkle1988tree,nakamoto2008bitcoin}.
When the pre-defined cumulative generation time limit is reached, the chunk at which the recorded limit is reached is designated for checkpointing, and the cumulative generation time resets (zeroes) for its dependencies.

Limits on chain length can be placed similarly, where instead of time, a maximum count of nodes forming the path back to the originating checkpoint can be taken, with checkpointing taking place upon reaching the limit.

Dynamic checkpointing, taking place after node creation, may be used to checkpoint upon certain memory thresholds being reached in the chunk host, with a full dump to disk.

My suggestion for the restoration of the system to current working state following node failure, from sparse checkpoints, can be performed in the following manner; If each node retains a record of the precise function used to create its chunk, along with references to the chunks required by the function (its immediate prerequisites), then it has an effective delta encoding to represent means of attaining one chunk from a prerequisite, and the graph of dependencies can be seen as a complete record of the construction of chunks, somewhat akin to a function-based, rather than line-based, git\cite{chacon2014progit}.
As such, any given node may be reconstructed by recursively walking back over the graph along the dependencies of nodes, collecting the required difference functions in a stack, until arriving at checkpoints, or leaf nodes representing file reads.
Upon reaching the checkpoints or leaves, ordered application of all of the accumulated difference functions through popping the stack, should result in the recreation of the node to be restored, assuming referential transparency.
Restoration from checkpoints serves effectively for enabling fault-tolerance in this respect.

\subsubsection{Self-Pruning of Dependency DAG's}

Up to this point, the graph as described has been append-only.
With such a description, it will grow excessively large, creating memory and traversal issues.
In conjunction with checkpointing, I consider a means of pruning the graph, keeping it to the minimum size necessary for recovery of the current state of the system.
While the checkpointing has not been implemented, the pruning has been implemented in a more brute-force manner (given in \cref{sec:gc}) than described in this section.

First, it is necessary to recognise that nodes representing unreferenced chunks still serve the bare purpose of delineating an intermediate (delta) transformation to target referenced chunks at some point further along the dependency path\cite{mogul2002deltahttp}.
If some checkpointed nodes are placed in the path of one of these unreferenced delta nodes and all of the delta's target referenced chunks, the unreferenced delta node is no longer necessary, and may be pruned.

The task then becomes one of determining the unreferenced delta nodes that have all their dependency paths, if followed backwards, resulting in a checkpoint.
A reference counting algorithm suffices to reveal these nodes, when combined with the important observation that checkpoints shouldn't count as references.
There are then four rules for algorithm for node removal: \begin{itemize} \item Every node has a target counter, as an integer, initialised at zero.
	\item The introduction of a target node must result in the unit incrementation of all of its direct prerequisite's target counters.
	\item The removal of a target node, or a node becoming a checkpoint, must result in the unit decrementation of all of its direct prerequisite's target counters.
	\item Unreferenced nodes with a target counter of zero are to be removed.
\end{itemize}
With these rules followed, unreferenced nodes with all targets resulting in checkpoints are immediately removed from the graph, thereby preserving the graph as being the minimum size required for restoration at the current point in time.

\subsection{Other Limitations}

Two other limitations are cloud elasticity, and security.

Cloud elasticity refers here to the ability to scale as needed within a cloud compute system, such as Amazon Web Services (AWS).
It is conceivable that, using AWS as an extended example, within a job running on Elastic Compute Service (ECS), upon needing more compute or memory, new instances could be dynamically spun up and form part of the network.
This would expand use-cases of \lsr{} beyond static and typically owned-metal use-cases, particularly as offices move away from desktop PC's.
The future of cloud HPC is enticing, and has barely been cracked -- the package \pkg{future.lambda} is a hint of the direction this could take.
Practically speaking, this is entirely possible, but requires the missing monitoring component - elasticity is already provided for out of the box due to the chunk location service.

Security is also a component that has been lower priority thus far in the development of \lsr{}, but needs to be considered with far greater seriousness if \lsr{} is to see wider use.
One manner by which security can be seen as an integral component of \lsr{} is if the framework factors access privaleges into its API.
\lsr{} effectively grants total remote code execution, so controlling access from within the framework would allow for finer constraints and greater trust of the packages.

One more limitation -- \R{}.
The \R{} programming language was chosen early on as a host for \lsr{}, for what seemed like obvious reasons.
\R{} is what most statisticians working with big data already know, and it is a flexible and capable language in its own right.
Through the course of this project there have been some cracks showing in this assumption, though it still seems the correct choice.
Especially in the past several years, \R{} has both visibly slipped in the (flawed) TIOBE index of use, as well as in relation to the primary alternative in the world of data science, \proglang{Python}.
With more users comes more bug reports and bug fixes, and it may very well be that the userbase of \lsr{} may be significantly larger, and with a more robust offering, had \proglang{Python} been used instead.
The incredible ecosystem of statistics-specific packages that \R{} offers would seem to outweigh this concern on its own, however.
The other aspect that is something of a crack in the assumption is the language itself.
In particular in this project, due to single threading and no concurrency, \R{} has served as something of a roadblock to clear description of network communications between nodes.
This was described in great detail in \cref{sec:concurrency-r}, and the end result was \orcv{} being mostly written in \proglang{C}, which was not too difficult, created extra complexity and loss of clarity, especially when compared with a language that may have such capabilities built-in.
A presentation on \lsr{} at the 2022 NZSA Unconference led to several audience questions.
Among them was Ross Ihaka, one of the co-creators of \R{}.
His question was simple -- ``Why \R{}?''\\
When prompted to expand, he responded, ``Why would you still use \R{}? It's just so 90's!''\\
There wasn't any possible response, other than acknowledgement that while imperfect, it has served well in the course of the project, and uniquely connects statisticians to work on problems in ways never though possible before.
The hope is that \lsr{} may open up one more way to interface with such problems, allowing for solutions that were also never thought possible before.
