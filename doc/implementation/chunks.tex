Chunks and computations on those chunks are represented as very simple S3 classes based on lists, with their structure given in \cref{fig:largerscale}

\fig{largerscale}{UML Diagram of Chunk and Computation classes}

Every chunk and computation has an identifier (``href'' in HTTP-language) - this is a UUID that allows for unambiguous specification of objects within the system.
Chunks contain an additional identifier of their generating process, that is, the identifier of the computation that is the result of.
Computations stored as data are typically significantly smaller than the data that results from them.
This implies that every single computation that takes place in the system can be stored as data and replicated across several nodes just as readily as their resulting data.
What this leads to is that if a node unexpectedly fails with some important resident data (the equivalence of data and computation being essential), the chunk referring to the data on that node maintains a reference to the generator of the data.
This generator will possibly exist on another that is still live, and the data/computation can in theory be regenerated.
