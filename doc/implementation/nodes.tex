The middle layer \chunknet{} defines the nodes and interactions between them, using an emulated \proglang{HTTP}, with the lower \orcv{} layer as its mechanism of communication.
The three nodes defined by this layer are the client, the worker, and the locator.

The client is the master node that the user interfaces with directly, and the main task of the client is to push individual chunks as data, to request remote calls on that data, and to pull the results of remote calls.
In order to push, remote compute, and pull, it connects to worker nodes, relaying the message for them to work on.
In order to connect to worker nodes, it has to know where they are located, given by their address and port.

Managing the knowledge of locations is the role of the locator service, which serves as a singular central database of addresses.
Existence as a node in the distributed system is synonymous with having a location stored in the locator service.
The locator service also performs the slightly orthogonal task of determining which chunks of data exist at what locations.

Worker nodes are the nodes that store data (chunks) and run computations upon them.
They respond to client requests, but importantly can communicate amongst themselves, particularly in the case of data being available on one worker node, with the data required for a computation taking place on a different worker node - the worker running the computation will request the data directly, thus functioning in a hybrid peer-to-peer fashion.
Communication among workers is dependent on the location service, in similar fashion to clients.

Both worker and locator nodes follow a similar architecture.
They both follow the same basic pattern of first initialising, (\cref{fig:node-init}), then running some initialisation function and endlessly repeating a check for the next event and handling that event.
The check for the next incoming event, as well as any response to the event, is the point of connection with the lower \orcv{} layer.
The nodes differ only in their core database schema, as well as the handlers associated with the HTTP requests sent to them as events.

The worker database schema is given in \cref{tab:wstore,tab:wstage,tab:waudience}, with an Entity Relationship Diagram given by \cref{fig:workerdb}

\widefig{node-init}{Initialisation of a node from a parent}

\tab{wstore}{Worker Store Table}

\tab{wstage}{Worker Stage Table}

\tab{waudience}{Worker Audience Table}

\fig{workerdb}{ER Diagram of worker entities}

The locator database schema is given in \cref{tab:lnodes,tab:ldata} with an Entity Relationship Diagram given by \cref{fig:locatordb}

\tab{lnodes}{Locator Nodes Table}

\tab{ldata}{Locator Data Table}

\fig{locatordb}{ER Diagram of locator entities}

The worker handlers are given by \cref{tab:whandler}

\tab{whandler}{Worker handlers for requests}

The locator handlers are given by \cref{tab:lhandler}

\tab{lhandler}{Locator handlers for requests}

\proglang{HTTP} is used as the communication protocol, because HTTP provides a reasonable constraint on remote calls, and is fairly universal.
Actual HTTP is not used currently; rather, it is emulated with an \R{} list composed of header and message body elements.

\subsubsection{Worker operation in detail}

Activity diagrams demonstrating the main operation of the worker is given in \cref{fig:workerops,fig:putcomp,fig:postdata,fig:putcompready,fig:getdata,fig:checkprereqs}.

The central event loop for a worker is shown at a high level in \cref{fig:workerops}.
Polling on a request, it listens for information regarding computation or data, each split into two forms.
The worker can either receive data, or be requested for data.
Not entirely symmetrically, a worker can either receive information that a computation is ready (all chunks are available for it to go ahead), or a new computation may be requested to run on the worker.
Each of these requests are handled in a different manner.

\widefig{workerops}{Activity diagram of main worker operation}

The handler that is launched in response to a request for a new computation to be run is given in \cref{fig:putcomp}.
The computation is deserialised and stored, and is then staged.
Here, certain checks need to take place regarding the availability of computation chunk arguments arguments as prerequisites.
Each prerequisite chunk needs to be locally available to the worker.
If all are available, then the chunks are marked as such and the computation may proceed.
Each missing prerequisite has an asynchronous data request issued to the host where that chunk is known to reside.
Notably, the worker doesn't block here.
After performing the sweep and issuing requests if necessary, the worker moves on immediately.

\widefig{putcomp}{Activities resulting from a PUT for computation}

A request for data is given in \cref{fig:getdata}.
When a request is received for data, the data may not yet be ready.
The data may still need to be computed, with the computation dependent on other prerequisites that are not yet available, either for the same reason, or because a requested machine is in the midst of computation.
As such, data that is not yet available yet has a request for it from some node has a placeholder kept for it in the worker cache, with the requesting node marked against it as part of the ``audience''.
The audience follow a subscribe model, where when the worker fills the placeholder with the data it will then send it to all of the audience for it, shown in \cref{fig:postdata}.

\widefig{getdata}{Activities resulting from a GET of data}

\Cref{fig:postdata} demonstrates the activities that occur when data is sent to a worker.
Immediately the data is stored.
There are two potential dependents on the presence of that data now: the audience of nodes who have asynchronously requested it, as well as any computations that depend upon it locally.
In the case of the audience, it is sent to them.
In the case of any dependent computations, its availability is marked.

\widefig{postdata}{Activities resulting from a POST of data}

The procedures described in \Cref{fig:putcomp,fig:postdata} both have the potential to alter the state of availability as marked on computational prerequisite chunks.
As such, following both of these, the same computation prerequisite check routine is run, as given by \cref{fig:checkprereqs}.
This is a simple sweep of prerequisites for computation, checking for their local availability.
For those that are entirely available, the computation is ready to proceed.
In keeping with the concurrent nature of the system, rather than monolithically engaging in the computations immediately, communication is sent from the worker to itself acknowledging the computations which are ready, and the event loop continues.

\widefig{checkprereqs}{Activities when checking computation prerequisites}

Upon receiving the information that a computation is ready, the worker performs a series of steps to run the computation and store the results, as given in \cref{fig:putcompready}.
Notably, it returns the data to itself through a request -- this is, as before, in keeping with the concurrent model.
In addition, it maintains the spirit of structured programming with a single entry and exit for a handler, allowing for a single point of engagement with the activities resulting from a sending of data as given in \cref{fig:postdata}, including sending the data to its audience and checks for data as prerequisite to computation.

\widefig{putcompready}{Activities resulting from a PUT of computation ready}

An example interaction in \cref{fig:sysinteract} shows the activities in the context of communication through data and computation requests.

\widefig{sysinteract}{Sequence diagram of example interaction between client, workers, and locator}
