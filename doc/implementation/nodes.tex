The middle layer \chunknet{} defines the nodes and interactions between them, using an emulated \proglang{HTTP}, with the lower \orcv{} layer as its mechanism of communication.
The three nodes defined by this layer are the client, the worker, and the locator.

The client is the master node that the user interfaces with directly, and the main task of the client is to push individual chunks as data, to request remote calls on that data, and to pull the results of remote calls.
In order to push, remote compute, and pull, it connects to worker nodes, relaying the message for them to work on.
In order to connect to worker nodes, it has to know where they are located, given by their address and port.

Managing the knowledge of locations is the role of the locator service, which serves as a singular central database of addresses.
Existence as a node in the distributed system is synonymous with having a location stored in the locator service.
The locator service also performs the slightly orthogonal task of determining which chunks of data exist at what locations.

Worker nodes are the nodes that store data (chunks) and run computations upon them.
They respond to client requests, but importantly can communicate amongst themselves, particularly in the case of data being available on one worker node, with the data required for a computation taking place on a different worker node - the worker running the computation will request the data directly, thus functioning in a hybrid peer-to-peer fashion.
Communication among workers is dependent on the location service, in similar fashion to clients.

Both worker and locator nodes follow a similar architecture.
They both follow the same basic pattern of first initialising, (\cref{fig:node-init}), then running some initialisation function and endlessly repeating a check for the next event and handling that event.
The check for the next incoming event, as well as any response to the event, is the point of connection with the lower \orcv{} layer.
The nodes differ only in their core database schema, as well as the handlers associated with the HTTP requests sent to them as events.

The worker database schema is given in \cref{tab:wstore,tab:wstage,tab:waudience}, with an Entity Relationship Diagram given by \cref{fig:workerdb}

\widefig{node-init}{Initialisation of a node from a parent}

\tab{wstore}{Worker Store Table}

\tab{wstage}{Worker Stage Table}

\tab{waudience}{Worker Audience Table}

\fig{workerdb}{ER Diagram of worker entities}

The locator database schema is given in \cref{tab:lnodes,tab:ldata} with an Entity Relationship Diagram given by \cref{fig:locatordb}

\tab{lnodes}{Locator Nodes Table}

\tab{ldata}{Locator Data Table}

\fig{locatordb}{ER Diagram of locator entities}

The worker handlers are given by \cref{tab:whandler}

\tab{whandler}{Worker handlers for requests}

The locator handlers are given by \cref{tab:lhandler}

\tab{lhandler}{Locator handlers for requests}

\proglang{HTTP} is used as the communication protocol, because HTTP provides a reasonable constraint on remote calls, and is fairly universal.
Actual HTTP is not used currently; rather, it is emulated with an \R{} list composed of header and message body elements.

\subsubsection{Worker operation in detail}

Activity diagrams demonstrating the main operation of the worker is given in \cref{fig:workerops,fig:putcomp,fig:postdata,fig:putcompready,fig:getdata,fig:checkprereqs}.
This is seen in use in an example interaction in \cref{fig:sysinteract}

\widefig{workerops}{Activity diagram of main worker operation}
\widefig{putcomp}{Activities resulting from a PUT for computation}
\widefig{postdata}{Activities resulting from a POST of data}
\widefig{putcompready}{Activities resulting from a PUT of computation ready}
\widefig{getdata}{Activities resulting from a GET of data}
\widefig{checkprereqs}{Activities when checking computation prerequisites}

\widefig{sysinteract}{Sequence diagram of example interaction between client, workers, and locator}
