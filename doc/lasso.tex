With the interface for the final system laid down in \cref{ch:ui}, We now move to the central problem that prompted this work; defining a novel distributed statistical algorithm with it.
Rather than detour with a truly novel algorithm, it is preferable to engage with something that is familiar, but holds a fairly generic form that novel analyses often share.

We will consider something based on the use on the Alternating Direction Method of Multipliers (ADMM), a technique for distributed convex optimisation.
In this case, part of the value of ADMM is its demonstrated generality.
The ADMM algorithm may serve as a means of expressing a surprising variety of statistical algorithms in a partitionable format, meaning that they can be distributed.
These include Regular and Sparse versions of Linear Regression, Generalised Linear Regression, Ridge and LASSO Regression, GAMs, SVMs, and many other well-used statistical algorithms.
Of these, the following example will demonstrate distributed LASSO regression, from ADMM, as described by \textcite{mateos2010}.
The choice of LASSO allows for a good mix of complexity and clarity, of these selections.

At its heart, ADMM decomposes a convex optimisation problem into many localised components, with iterative co-ordination of information between them.
In this way, it is well suited to distributed computation, due to the ability to allow useful work to take place on subproblems made independent in between co-ordination events.
A richer description of ADMM, with significant background, is given in \textcite{boyd2011}.

For this chapter, it will suffice to take the results of \textcite{boyd2011} in stating the constraint of ADMM being to solve problems of the form given in \cref{eq:admm-form}.

\eq{admm-form}

From this, the augmented lagrangian is formed by \cref{eq:lagrangian}.

\eq{lagrangian}

Then ADMM consists of the iterations given by \cref{eq:admm-iter}.

\eq{admm-iter}

The most important aspect of ADMM for our distributed purposes is that it doesn't just work randomly -- \(f\) and \(g\) must be separable, i.e.,

\eq{f-sep}

And analogously for \(g\).
With this particular demonstration of distributed LASSO dependent on proofs of separability for partitioned block matrices using rows as observations, and minor ADMM optimisations, as described in \textcite{boyd2011}.

The general form of ADMM having been described, we move to the specific case of LASSO.
We begin by assessing the mathematical form in \cref{sec:mathlasso}, followed by the ``standard R'' means of writing a chunked algorithm in contraposition with the \lsr{} manner in \cref{sec:rlasso}.

\section{Distributed LASSO Mathematical Definition}\label{sec:mathlasso}

This section seeks to give a brief taste of what a mathematical formulation for a distributed statistical model takes, in order to demonstrate the semantic and syntactic similarity to \lsr{} in \cref{sec:rlasso}.

We begin with a description of the input data, as given in \cref{eq:mathlassodata}

\eq{mathlassodata}

The starting data includes a column block matrix of explanatory variables, $A$, consisting of $N$ sub-matrices.
This is equivalent to a distributed object consisting of $N$ chunks.
Here, each chunk is of the standard form where rows are individual observations and columns are variables.
We also have a block matrix $b$ of the same number of chunks, with each chunk being the column vector of response variables to the corresponding $A$ chunks.

The standard form of the LASSO as an optimisation problem is expressed in \cref{eq:mathlassointention}.

\eq{mathlassointention}

The body of the ADMM loop is given by \cref{eq:mathlassoloop}.
Of note is the complexity, the presence of iteration, and the interactions between sets of chunks getting reduced and emerged.

\eq{mathlassoloop}

\section{Distributed LASSO R Description}\label{sec:rlasso}

This section gives both base \R{} syntax for working with a local chunked dataset, as well as the minimal changes that are required when using \lsr{} to transform the expressions to handle distributed data.
The core substance of this section is to demonstrate the ease with which \R{} is able to meet a mathematical definition, and the successive ease by which \lsr{} is able to turn that into a truly distributed algorithm.
As in \cref{sec:mathlasso}, the working example is given of distributed LASSO.
Consider first some chunked data, given as a diff in \cref{tab:chunked-comp}.
In the diff, semantic differences are demarcated through underlining, with shared code given centrally, with no dividing line.
On the left of the diff we have how the LASSO as described might be encoded in the absence of the API, assuming that the data fit into memory, and on the right, we make use of large scale \R{} with no such constraining assumption.

In the \lsr{} code, distributed data may come from multiple files and multiple hosts holding the chunks, and this is easily provided for.
The distribution comes with the necessity to carefully differentiate between the reference of the distributed object, and the distributed object itself.
The ability to explicitly distribute local values to particular locations is also demonstrated here.

\tab{chunked-comp}{Comparison of reading in chunks of data}

The layout of the data is followed by the iterative loop, given in \cref{tab:loop-comp}.
Within the iterative loop, we can see that very little is actually needed to be changed in order to distribute this algorithm.
We make use of a function that operates on distributed objects which we define ourselves in the successive diff, as well as the emerge to bring the distributed local as we saw before.

\tab{loop-comp}{Comparison of the iterative loop}

Note the significantly simplified and reduced logic in switching to \lsr{}, which bears a far closer resemblance to the mathematical description.
The \code{x-update} function given above is exemplary of the approach provided by \lsr{}, which allows for the switching of a local to a distributed function through the higher-order \code{dist} function, as demonstrated in \cref{tab:x-update}.

\tab{x-update}{Comparison of the \code{x-update} functions}

These listings, combined, serve to define a distributed LASSO model.
