The working statistician is presented with a staggering array of varying datasets from which to attain insight.
The data varies along many different fronts, including complexity, shape, velocity, and size, among others.
In this chapter, we seek to compose a motivating example with which to illustrate the development of our answer to the problem of large data.

As an example dataset, we consider only data which exists beyond the regular spectrum of size, sitting larger than memory.
Concretely, for the purpose of example, consider the NYC TLC dataset\cite{tlc2021trips}.
This dataset consists of measures relating to all taxi and limousine trips in New York City from January 2009 to present.
There are roughly 2 billion rows and 25 columns, totalling over 60Gb.
Measures include aspects of the trip such as pickup and dropoff times and locations, passenger count, tips and tolls, and vendor.
The dataset is notoriously messy for such a large tabular format; locations are censored from midway through the dataset for privacy reasons, many fields depend entirely on others, etc. It is these qualities in particular that make it valuable as an example for analysis; most tools don't make it easy for users to explore such issues in data, and analysis is hindered - an outcome of this project should be to make such exploration easy.

With such a dataset, many questions may be asked of it.
Geographic questions, such as where hotspots in dropoff and pickup are, or if there is any clustering when looked at in tandem.
Behavioural questions, such as tipping and the relation to other variables.
The dataset has many variables that could serve as foreign keys with which to link to other, smaller datasets, in order to gain more insight, such as local events taking place on particular dates, etc.

A full analysis is to be performed on the dataset, involving all the standard components: hypotheses, exploratory data analysis, etc., culminating in the use of some advanced, as-yet unimplemented statistical technique.
Given the previous methodologies explored in the previous literature review chapter, let's consider in detail just what such a technique should be; notably, it should serve to give issue to all of them, as a prompt to develop upon for our platform.
Ideally, this would be a research-level algorithm for statistical analysis, having no pre-existing implementations offered at the user level by any of the platforms explored.
There are some issues with such an approach, however, in that cutting-edge statistical techniques require a highly localised understanding and are typically extremely complex, with implementation being very specific to the technique.
This brings the risk that the illustration will draw more attention to itself than to the platform it seeks to illustrate.
As such, we instead make use of an approach well-known to statisticians, the Generalised Linear Model, implemented through Reweighted Least Squares.
Such a model is well-understood, and the implementation sufficiently straightforward for the sake of clarity, and possessed enough features which enable it to be analogised to a broad variety of other statistical algorithms.
The intention is to allow this example to stand in for the use-case of a research algorithm as applied to the dataset, with appropriate analogies to be drawn throughout.
In later chapters, following the construction of such the platform, it will be put to use in the actual implementation of advanced statistical algorithms, to prove the concept.
Therefore, we consider as our motivating example the NYC taxi dataset, to be analysed using a LASSO, as the proxy for an unimplemented statistical modelling algorithm.

With such an example as a background illumination, we will build from scratch a tool which can operate in a way that no other can, and perform complex analyses with ease.

Our first task will require consideration of the very \textbf{representation} of the data.
Being such a large dataset, it will necessarily pose problems to any naive implementation.
Furthermore, assuming the existence of some means of representing the data, we will need to operate upon it, in order to gain any insight from it - this is at the \textbf{high level}.
At a \textbf{lower level}, we will need to determine the operations available to us in order to easily and efficiently implement the complex algorithms with which we use to perform the analysis.
Importantly, it should be just as easy to implement a new statistical algorithm as it is to use a pre-implemented one.

For example, consider that we have the data as its raw form directly from the TLC website, as a collated \file{csv} file on disk.
How do we begin?

\src{massive-crash}{Naive read of larger than memory data guaranteeing an out-of-memory error}

The call in \Cref{lst:massive-crash} simply won't work.
The object in memory will cause an out-of-memory error even in high-end PC's, at its 60Gb+ size.
There must be a better means of representing the data than its direct form as standard \R{} objects.
Conversely, such a representation can't possibly stray too far from what the statistician is familiar with in the language.
Assume, however, that it does somehow exist, and can be treated like a regular object.
For example, we may have it represented as a dataframe named \code{nyc-taxi}, with columns to be accessed through the \code{dollar} operator and the name of the column.
How do we perform basic operations upon it?
For instance, how might we determine the largest tip given?

\src{max-tip}{Typical determination of maximum in R}

\cref{lst:max-tip} is the standard means of interaction, but this operation is highly
dependent on the means of data representation, and will require some
discussion of the scaffolding required to make it work.
And based on this, what are the operations that can be depended upon in order to create such scaffolding?
In partial response to the first statement, there is some knowledge at least of data representation - that of smaller subsets of data, which may be read into memory.
These subsets allow for some operations to be performed, each providing a window into some part of the whole dataset.
With all subsets of a dataset operated upon independently, for some operations this is equivalent to performing an operation over an entire dataset.
These subsets are referred to as \textbf{chunks}, and will be described in greater detail in the following chapter.
For now, consider the chunks as containers for a subset of the data, which in itself requires certain operations in order to interact with.
For instance, some operations required are to unmarshall the data out of the chunk, and perform operations on data within the chunk - reading and writing, as minimum necessary features.
The \textbf{relation between chunks} within a \textbf{group of chunks} at a higher level is also worth bearing some thought.
For instance, when a set of chunks are treated as a cohesive object, and the underlying data of the object is sought, how best to retain the shape of the dataset cohesively, without leaving any distortion from the chunks?
For example, given that at a high level, chunks are to be forgotten, how best to avoid artifacts of subset formation?
Consider \cref{lst:chunk-object} where \code{d1} refers to a group of three chunks treated as one object:

\src[Rout]{chunk-object}{Group of chunks in a collection as a single object}

The \code{d} stands for ``\textbf{distributed}''; chunks must not occupy memory all together, otherwise memory will run out -- therefore, they must be distributed across a set of computers, or accessed sequentially within one computer.
Each chunk holds only one single integer for the sake of simplicity of example.
When the integers are unmarshalled out of each chunk, or ``\textbf{emerged}'', it may be expected that a cohesive integer vector of length three should result, here given as variable \code{l1} in \cref{lst:emerged-chunks}.
In this case, \code{l} stands for ``\textbf{local}''; that is, local to the operating environment, as any regular object, without the indirection created by chunks.

\src[Rout]{emerged-chunks}{Chunks emerged to a cohesive small object}

Were the chunks to be treated as mere containers for a single underlying object, this is the desired behaviour.
Failing this, without a means of defined relation between a group of chunks, without any manner of combination, the following behaviour may result instead, where a group of emerged chunks retains the structure imposed by chunking, as a list or similar, as in \cref{lst:emerged-chunks-structured}:

\src[Rout]{emerged-chunks-structured}{Chunks emerged to a small object which retains the chunked structure}

These requirements can be summarised as the considerations concerning an object system, and are therefore described in greater detail in \cref{sec:object-system}.
